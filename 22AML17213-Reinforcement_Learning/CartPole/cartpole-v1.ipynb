{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed35df2",
   "metadata": {},
   "source": [
    "# CartPole: Balance a Pole on a Moving Cart Applying Q-Learning & Q-Network\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"./images/cart_pole.gif\" width=\"300\" alt=\"CartPole Environment\">\n",
    "</div>\n",
    "\n",
    "This notebook demonstrates how to create and interact with the CartPole environment using OpenAI Gymnasium. The CartPole problem is a classic control problem where we need to balance a pole on a moving cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a4d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import gymnasium as gym                     # The CartPole environment\n",
    "from gymnasium.wrappers import RecordVideo  # For recording the video of the episode\n",
    "import time                                 # For adding delays in visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea0ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action space:  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Creates the CartPole environment along with visual rendering of during the episode\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')  # rgb_array needed for video recording\n",
    "\n",
    "\"\"\"\n",
    "# Creates videos of the agent's performance in the specified folder\n",
    "env = RecordVideo(env, \n",
    "                  video_folder= 'videos',           # Folder to save videos\n",
    "                  name_prefix=\"eval\",               # Prefix for video file names\n",
    "                  episode_trigger=lambda x: True)   # Trigger to record every episode\n",
    "\"\"\"\n",
    "\n",
    "# Shows observation space of the environment\n",
    "# The observation space consists of 4 continuous values:\n",
    "# 1. Cart Position: [-4.8, 4.8] - Position of cart on track\n",
    "# 2. Cart Velocity: [-Inf, Inf] - Velocity of the cart\n",
    "# 3. Pole Angle: [-0.418 rad (-24°), 0.418 rad (24°)] - Angle of the pole\n",
    "# 4. Pole Angular Velocity: [-Inf, Inf] - Rate of change of the angle\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "\n",
    "# Shows action space of the environment: 2 discrete actions - 0: Move cart to left, 1: Move cart to right\n",
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c877923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reads the post-initialization observation of the environment:\n",
      "Cart position: 0.02739560417830944, Cart velocity: -0.006112155970185995, Pole angle: 0.03585979342460632, Pole angular velocity: 0.019736802205443382\n"
     ]
    }
   ],
   "source": [
    "# Resets the environment to start a new episode\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "print(\"Reads the post-initialization observation of the environment:\")\n",
    "print(f\"Cart position: {observation[0]}, Cart velocity: {observation[1]}, Pole angle: {observation[2]}, Pole angular velocity: {observation[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588932a2",
   "metadata": {},
   "source": [
    "## Balancing the Pole with Random Actions\n",
    "\n",
    "The pole will now be tried to be balanced with random actions.\n",
    "The following steps will be performed in each step:\n",
    "1. Sample a random action (0 = move left, 1 = move right)\n",
    "2. Apply the action to the environment\n",
    "3. Get the new observation and reward\n",
    "4. Render the environment\n",
    "5. Check if episode is over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6240d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Episode is completed. Total reward: 30.0, Steps taken: 30\n"
     ]
    }
   ],
   "source": [
    "# Wraps the environment to record statistics for one episode\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=1)\n",
    "\n",
    "episode_over = False    # Track if the episode is over to stop the loop\n",
    "# total_reward = 0        # Accumulates rewards for the episode\n",
    "# step_count = 0          # Initialize step counter\n",
    "\n",
    "while not episode_over:\n",
    "\n",
    "    action = env.action_space.sample()  # Samples a discrete random action: 0 (move left) or 1 (move right)\n",
    "    print(f\"Action taken: {action}\")    # Prints the action taken\n",
    "    \n",
    "    # step_count += 1                     # Increments step counter\n",
    "\n",
    "    # Takes a step in the environment with the sampled action\n",
    "    # Returns five values:\n",
    "    # 1. observation: Array of 4 values [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
    "    # 2. reward: +1 for every step taken, including the terminal state\n",
    "    # 3. terminated: True if pole angle > ±12° or cart position > ±2.4 units\n",
    "    # 4. truncated: True if episode length >= 500 timesteps (default in CartPole)\n",
    "    # 5. info: Additional information (empty dictionary in CartPole)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    #env.render()   # Renders the environment to visualize the agent's performance\n",
    "\n",
    "    # Accumulates the rewards for the episode. It is +1 for each time step the pole remains upright\n",
    "    # total_reward += reward\n",
    "\n",
    "    # Checks if the episode is over\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "    # time.sleep(0.2) # Adds a delay to visualize the rendering better\n",
    "\n",
    "print(f\"Episode is completed. Total reward: {env.return_queue[0]}, Steps taken: {env.length_queue[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0e6d3",
   "metadata": {},
   "source": [
    "Now, an intelligent agent will be build to keep the pole balanced for longer period of time resulting larger total rewards over each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1807d4",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Finally, let's close the environment to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0b3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e624676",
   "metadata": {},
   "source": [
    "## The Q-Learning Agent\n",
    "\n",
    "An agent gets trained using Q-learning to develop a policy that tells the agent which action to take in each situation to maximize long-term rewards. The key components are:\n",
    "\n",
    "### State Space Discretization\n",
    "Since CartPole has continuous state space, so it is discretized into bins as follows.\n",
    "- Cart Position: [-4.8, 4.8] → 8 bins\n",
    "- Cart Velocity: [-4, 4] → 8 bins\n",
    "- Pole Angle: [-0.418, 0.418] → 8 bins\n",
    "- Pole Angular Velocity: [-4, 4] → 8 bins\n",
    "\n",
    "### Q-Learning Components\n",
    "1. **Q-Table**: Maps state-action pairs to expected rewards\n",
    "2. **ε-greedy Policy**: Balance exploration and exploitation\n",
    "3. **Learning Rate (α)**: Controls how much new information overrides old\n",
    "4. **Discount Factor (γ)**: Balances immediate vs future rewards\n",
    "5. **Exploration Decay**: Gradually reduce random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb4d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class CartPoleAgent:\n",
    "    \"\"\"\n",
    "    Q-learning agent for the CartPole environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.95, \n",
    "                 initial_epsilon=1.0, epsilon_decay=0.995, final_epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent for the CartPole-v1 environment.\n",
    "        \n",
    "        Args:\n",
    "            env: The CartPole-v1 environment\n",
    "            learning_rate: How quickly the agent learns from new experiences (α)\n",
    "            discount_factor: How much future rewards are valued (γ)\n",
    "            initial_epsilon: Starting exploration rate\n",
    "            epsilon_decay: Rate at which exploration decreases\n",
    "            final_epsilon: Minimum exploration rate\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        \n",
    "        # Defines state space discretization\n",
    "        self.n_bins = 8  # Number of bins for each dimension\n",
    "        self.state_bins = {\n",
    "            'cart_position': np.linspace(-4.8, 4.8, self.n_bins),\n",
    "            'cart_velocity': np.linspace(-4, 4, self.n_bins),\n",
    "            'pole_angle': np.linspace(-0.418, 0.418, self.n_bins),\n",
    "            'pole_angular_velocity': np.linspace(-4, 4, self.n_bins)\n",
    "        }\n",
    "        \n",
    "        # Initialize Q-table with optimistic initial values to encourage exploration\n",
    "        self.q_table = defaultdict(lambda: np.ones(env.action_space.n))\n",
    "        \n",
    "        # Track learning progress\n",
    "        self.training_rewards = []\n",
    "        \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"\n",
    "        Convert continuous state values to discrete state indices.\n",
    "        \n",
    "        Args:\n",
    "            observation: [cart_pos, cart_vel, pole_angle, pole_ang_vel]\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Discretized state representation\n",
    "        \"\"\"\n",
    "        discretized = []\n",
    "        for i, (name, bins) in enumerate(self.state_bins.items()):\n",
    "            discretized.append(np.digitize(observation[i], bins))\n",
    "        return tuple(discretized)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Choose action using ε-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (should be already discretized)\n",
    "            \n",
    "        Returns:\n",
    "            int: Chosen action (0: left, 1: right)\n",
    "        \"\"\"\n",
    "    \n",
    "        if np.random.random() < self.epsilon:       # Explores by choosing random action\n",
    "            return self.env.action_space.sample()\n",
    "    \n",
    "        return np.argmax(self.q_table[state])       # Exploits by choosing best action based on Q-values\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, terminated):\n",
    "        \"\"\"\n",
    "        Updates Q-value for state-action pair using the Bellman equation.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            terminated: Whether episode ended\n",
    "        \"\"\"\n",
    "\n",
    "        # Get best future value (0 if terminated)\n",
    "        best_next_value = 0 if terminated else np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Calculate target Q-value using Bellman equation\n",
    "        target = reward + self.discount_factor * best_next_value\n",
    "        \n",
    "        # Update Q-value towards target using learning rate\n",
    "        current_q = self.q_table[state][action]\n",
    "        self.q_table[state][action] = current_q + self.learning_rate * (target - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate by multiplication factor\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, \n",
    "                          self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def train_episode(self):\n",
    "        \"\"\"\n",
    "        Train the agent for one episode.\n",
    "        \n",
    "        Returns:\n",
    "            float: Total reward for the episode\n",
    "        \"\"\"\n",
    "        state, _ = self.env.reset()\n",
    "        state = self.discretize_state(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Choose and take action\n",
    "            action = self.get_action(state)\n",
    "            next_observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            \n",
    "            # Process new state and update Q-value\n",
    "            next_state = self.discretize_state(next_observation)\n",
    "            self.update_q_value(state, action, reward, next_state, terminated)\n",
    "            \n",
    "            # Update state and accumulate reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Decay exploration rate after episode\n",
    "        self.decay_epsilon()\n",
    "        self.training_rewards.append(total_reward)\n",
    "        \n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9692896",
   "metadata": {},
   "source": [
    "### Training the Q-Learning Agent\n",
    "\n",
    "Let's create an instance of our Q-learning agent and train it over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e09f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/1000\n",
      "Average Reward (last 100 episodes): 22.80\n",
      "Current Exploration Rate (ε): 0.606\n",
      "--------------------\n",
      "Episode 200/1000\n",
      "Average Reward (last 100 episodes): 25.20\n",
      "Current Exploration Rate (ε): 0.367\n",
      "--------------------\n",
      "Episode 300/1000\n",
      "Average Reward (last 100 episodes): 33.99\n",
      "Current Exploration Rate (ε): 0.222\n",
      "--------------------\n",
      "Episode 400/1000\n",
      "Average Reward (last 100 episodes): 71.07\n",
      "Current Exploration Rate (ε): 0.135\n",
      "--------------------\n",
      "Episode 500/1000\n",
      "Average Reward (last 100 episodes): 85.02\n",
      "Current Exploration Rate (ε): 0.082\n",
      "--------------------\n",
      "Episode 600/1000\n",
      "Average Reward (last 100 episodes): 85.80\n",
      "Current Exploration Rate (ε): 0.049\n",
      "--------------------\n",
      "Episode 700/1000\n",
      "Average Reward (last 100 episodes): 94.92\n",
      "Current Exploration Rate (ε): 0.030\n",
      "--------------------\n",
      "Episode 800/1000\n",
      "Average Reward (last 100 episodes): 84.39\n",
      "Current Exploration Rate (ε): 0.018\n",
      "--------------------\n",
      "Episode 900/1000\n",
      "Average Reward (last 100 episodes): 92.12\n",
      "Current Exploration Rate (ε): 0.011\n",
      "--------------------\n",
      "Episode 1000/1000\n",
      "Average Reward (last 100 episodes): 94.71\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Create the environment and wrap it to record statistics\n",
    "env = gym.make('CartPole-v1')\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "\n",
    "# Create the agent\n",
    "agent = CartPoleAgent(env, \n",
    "             learning_rate=0.1,          # Learning rate\n",
    "             discount_factor=0.95,       # Discount factor\n",
    "             initial_epsilon=1.0,        # Initial exploration rate\n",
    "             epsilon_decay=0.995,        # Exploration decay rate\n",
    "             final_epsilon=0.01)         # Minimum exploration rate\n",
    "\n",
    "# Training parameters\n",
    "n_episodes = 1000\n",
    "print_interval = 100\n",
    "\n",
    "# Train the agent\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = agent.train_episode()\n",
    "    \n",
    "    # Print progress every print_interval episodes\n",
    "    if (episode + 1) % print_interval == 0:\n",
    "        avg_reward = np.mean(agent.training_rewards[-print_interval:])\n",
    "        print(f\"Episode {episode + 1}/{n_episodes}\")\n",
    "        print(f\"Average Reward (last {print_interval} episodes): {avg_reward:.2f}\")\n",
    "        print(f\"Current Exploration Rate (ε): {agent.epsilon:.3f}\")\n",
    "        print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87defec4",
   "metadata": {},
   "source": [
    "### Evaluating the Trained Q-Learning Agent\n",
    "\n",
    "Now let's see how well our trained agent performs. We'll run a few evaluation episodes with rendering enabled and no exploration (ε = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455712af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 1: Reward = 98.0, Steps = 98\n",
      "Evaluation Episode 2: Reward = 87.0, Steps = 87\n",
      "Evaluation Episode 3: Reward = 93.0, Steps = 93\n",
      "Evaluation Episode 4: Reward = 101.0, Steps = 101\n",
      "Evaluation Episode 5: Reward = 97.0, Steps = 97\n",
      "\n",
      "Average Evaluation Reward: 95.20\n"
     ]
    }
   ],
   "source": [
    "# Create environment for evaluation with rendering\n",
    "eval_env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Run 5 evaluation episodes\n",
    "n_eval_episodes = 5\n",
    "eval_rewards = []\n",
    "eval_episode_steps = []\n",
    "\n",
    "for episode in range(n_eval_episodes):\n",
    "    state, _ = eval_env.reset()\n",
    "    state = agent.discretize_state(state)\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Use the learned policy (no exploration)\n",
    "        action = np.argmax(agent.q_table[state])\n",
    "        \n",
    "        # Take action and get new state\n",
    "        next_observation, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "        next_state = agent.discretize_state(next_observation)\n",
    "        \n",
    "        # Update state and reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Add a small delay to better visualize the agent's behavior\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "    eval_episode_steps.append(episode_steps)\n",
    "    print(f\"Evaluation Episode {episode + 1}: Reward = {episode_reward}, Steps = {episode_steps}\")\n",
    "\n",
    "print(f\"\\nAverage Evaluation Reward: {np.mean(eval_rewards):.2f}\")\n",
    "\n",
    "# Close the evaluation environment\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b890a3",
   "metadata": {},
   "source": [
    "It shows that averaged rewards received by the trained agent is much more than the average rewards received when just random actions were taken as experimented in the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc7c6b",
   "metadata": {},
   "source": [
    "## The Q-Network Agent\n",
    "\n",
    "Implements a Q-Network agent using TensorFlow. Q-Network is better suited for environments with continuous state spaces like CartPole because it eliminates the need for for state discretization, generalizes across similar states better and captures complex state-action relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class QNAgent:\n",
    "    \"\"\"The Q-Network Agent for CartPole-v1 environment.\"\"\"\n",
    "    def __init__(self, state_size, action_size, \n",
    "                 learning_rate=0.001, discount_factor=0.95,\n",
    "                 initial_epsilon=1.0, epsilon_decay=0.995, final_epsilon=0.01,\n",
    "                 memory_size=10000, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initializes the Q-Network Agent.\n",
    "        \n",
    "        Args:\n",
    "            state_size: Dimension of state space\n",
    "            action_size: Dimension of action space\n",
    "            learning_rate: Learning rate for the neural network\n",
    "            discount_factor: Discount factor for future rewards\n",
    "            initial_epsilon: Initial exploration rate\n",
    "            epsilon_decay: Rate at which exploration decreases\n",
    "            final_epsilon: Minimum exploration rate\n",
    "            memory_size: Size of experience replay buffer\n",
    "            batch_size: Number of samples to train on in each update\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = final_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Creates main and target networks\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        \n",
    "        # Training metrics\n",
    "        self.training_rewards = []\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Builds a neural network model.\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copies weights from main model to target model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores experience in replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Chooses action using ε-greedy policy.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        \n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Trains the network on a batch of experiences.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Samples random batch from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        \n",
    "        # Separates experiences into arrays\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            states[i] = state\n",
    "            next_states[i] = next_state\n",
    "        \n",
    "        # Predicts Q-values for current and next states\n",
    "        target_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        # Updates target Q-values with Bellman equation\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.gamma * np.max(target_q_values[i])\n",
    "            current_q_values[i][action] = target\n",
    "        \n",
    "        # Trains the model\n",
    "        self.model.fit(states, current_q_values, epochs=1, verbose=0)\n",
    "        \n",
    "        # Decays exploration rate\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Trains for one episode.\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Chooses and take action\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, self.state_size])\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Stores experience and train\n",
    "            self.remember(state[0], action, reward, next_state[0], done)\n",
    "            self.replay()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        self.training_rewards.append(total_reward)\n",
    "\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb5fd34",
   "metadata": {},
   "source": [
    "### Training the Q-Network Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e04cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100\n",
      "Average Reward (last 1 episodes): 10.00\n",
      "Current Exploration Rate (ε): 1.000\n",
      "--------------------\n",
      "Episode 2/100\n",
      "Average Reward (last 1 episodes): 23.00\n",
      "Current Exploration Rate (ε): 0.990\n",
      "--------------------\n",
      "Episode 2/100\n",
      "Average Reward (last 1 episodes): 23.00\n",
      "Current Exploration Rate (ε): 0.990\n",
      "--------------------\n",
      "Episode 3/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.909\n",
      "--------------------\n",
      "Episode 3/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.909\n",
      "--------------------\n",
      "Episode 4/100\n",
      "Average Reward (last 1 episodes): 23.00\n",
      "Current Exploration Rate (ε): 0.810\n",
      "--------------------\n",
      "Episode 4/100\n",
      "Average Reward (last 1 episodes): 23.00\n",
      "Current Exploration Rate (ε): 0.810\n",
      "--------------------\n",
      "Episode 5/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.744\n",
      "--------------------\n",
      "Episode 5/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.744\n",
      "--------------------\n",
      "Episode 6/100\n",
      "Average Reward (last 1 episodes): 14.00\n",
      "Current Exploration Rate (ε): 0.694\n",
      "--------------------\n",
      "Episode 6/100\n",
      "Average Reward (last 1 episodes): 14.00\n",
      "Current Exploration Rate (ε): 0.694\n",
      "--------------------\n",
      "Episode 7/100\n",
      "Average Reward (last 1 episodes): 22.00\n",
      "Current Exploration Rate (ε): 0.621\n",
      "--------------------\n",
      "Episode 7/100\n",
      "Average Reward (last 1 episodes): 22.00\n",
      "Current Exploration Rate (ε): 0.621\n",
      "--------------------\n",
      "Episode 8/100\n",
      "Average Reward (last 1 episodes): 11.00\n",
      "Current Exploration Rate (ε): 0.588\n",
      "--------------------\n",
      "Episode 8/100\n",
      "Average Reward (last 1 episodes): 11.00\n",
      "Current Exploration Rate (ε): 0.588\n",
      "--------------------\n",
      "Episode 9/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.540\n",
      "--------------------\n",
      "Episode 9/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.540\n",
      "--------------------\n",
      "Episode 10/100\n",
      "Average Reward (last 1 episodes): 45.00\n",
      "Current Exploration Rate (ε): 0.431\n",
      "--------------------\n",
      "Episode 10/100\n",
      "Average Reward (last 1 episodes): 45.00\n",
      "Current Exploration Rate (ε): 0.431\n",
      "--------------------\n",
      "Episode 11/100\n",
      "Average Reward (last 1 episodes): 44.00\n",
      "Current Exploration Rate (ε): 0.346\n",
      "--------------------\n",
      "Episode 11/100\n",
      "Average Reward (last 1 episodes): 44.00\n",
      "Current Exploration Rate (ε): 0.346\n",
      "--------------------\n",
      "Episode 12/100\n",
      "Average Reward (last 1 episodes): 19.00\n",
      "Current Exploration Rate (ε): 0.314\n",
      "--------------------\n",
      "Episode 12/100\n",
      "Average Reward (last 1 episodes): 19.00\n",
      "Current Exploration Rate (ε): 0.314\n",
      "--------------------\n",
      "Episode 13/100\n",
      "Average Reward (last 1 episodes): 8.00\n",
      "Current Exploration Rate (ε): 0.302\n",
      "--------------------\n",
      "Episode 13/100\n",
      "Average Reward (last 1 episodes): 8.00\n",
      "Current Exploration Rate (ε): 0.302\n",
      "--------------------\n",
      "Episode 14/100\n",
      "Average Reward (last 1 episodes): 20.00\n",
      "Current Exploration Rate (ε): 0.273\n",
      "--------------------\n",
      "Episode 14/100\n",
      "Average Reward (last 1 episodes): 20.00\n",
      "Current Exploration Rate (ε): 0.273\n",
      "--------------------\n",
      "Episode 15/100\n",
      "Average Reward (last 1 episodes): 10.00\n",
      "Current Exploration Rate (ε): 0.260\n",
      "--------------------\n",
      "Episode 15/100\n",
      "Average Reward (last 1 episodes): 10.00\n",
      "Current Exploration Rate (ε): 0.260\n",
      "--------------------\n",
      "Episode 16/100\n",
      "Average Reward (last 1 episodes): 16.00\n",
      "Current Exploration Rate (ε): 0.240\n",
      "--------------------\n",
      "Episode 16/100\n",
      "Average Reward (last 1 episodes): 16.00\n",
      "Current Exploration Rate (ε): 0.240\n",
      "--------------------\n",
      "Episode 17/100\n",
      "Average Reward (last 1 episodes): 14.00\n",
      "Current Exploration Rate (ε): 0.223\n",
      "--------------------\n",
      "Episode 17/100\n",
      "Average Reward (last 1 episodes): 14.00\n",
      "Current Exploration Rate (ε): 0.223\n",
      "--------------------\n",
      "Episode 18/100\n",
      "Average Reward (last 1 episodes): 22.00\n",
      "Current Exploration Rate (ε): 0.200\n",
      "--------------------\n",
      "Episode 18/100\n",
      "Average Reward (last 1 episodes): 22.00\n",
      "Current Exploration Rate (ε): 0.200\n",
      "--------------------\n",
      "Episode 19/100\n",
      "Average Reward (last 1 episodes): 15.00\n",
      "Current Exploration Rate (ε): 0.186\n",
      "--------------------\n",
      "Episode 19/100\n",
      "Average Reward (last 1 episodes): 15.00\n",
      "Current Exploration Rate (ε): 0.186\n",
      "--------------------\n",
      "Episode 20/100\n",
      "Average Reward (last 1 episodes): 32.00\n",
      "Current Exploration Rate (ε): 0.158\n",
      "--------------------\n",
      "Episode 20/100\n",
      "Average Reward (last 1 episodes): 32.00\n",
      "Current Exploration Rate (ε): 0.158\n",
      "--------------------\n",
      "Episode 21/100\n",
      "Average Reward (last 1 episodes): 20.00\n",
      "Current Exploration Rate (ε): 0.143\n",
      "--------------------\n",
      "Episode 21/100\n",
      "Average Reward (last 1 episodes): 20.00\n",
      "Current Exploration Rate (ε): 0.143\n",
      "--------------------\n",
      "Episode 22/100\n",
      "Average Reward (last 1 episodes): 16.00\n",
      "Current Exploration Rate (ε): 0.132\n",
      "--------------------\n",
      "Episode 22/100\n",
      "Average Reward (last 1 episodes): 16.00\n",
      "Current Exploration Rate (ε): 0.132\n",
      "--------------------\n",
      "Episode 23/100\n",
      "Average Reward (last 1 episodes): 13.00\n",
      "Current Exploration Rate (ε): 0.124\n",
      "--------------------\n",
      "Episode 23/100\n",
      "Average Reward (last 1 episodes): 13.00\n",
      "Current Exploration Rate (ε): 0.124\n",
      "--------------------\n",
      "Episode 24/100\n",
      "Average Reward (last 1 episodes): 20.00\n",
      "Current Exploration Rate (ε): 0.112\n",
      "--------------------\n",
      "Episode 24/100\n",
      "Average Reward (last 1 episodes): 20.00\n",
      "Current Exploration Rate (ε): 0.112\n",
      "--------------------\n",
      "Episode 25/100\n",
      "Average Reward (last 1 episodes): 34.00\n",
      "Current Exploration Rate (ε): 0.094\n",
      "--------------------\n",
      "Episode 25/100\n",
      "Average Reward (last 1 episodes): 34.00\n",
      "Current Exploration Rate (ε): 0.094\n",
      "--------------------\n",
      "Episode 26/100\n",
      "Average Reward (last 1 episodes): 21.00\n",
      "Current Exploration Rate (ε): 0.085\n",
      "--------------------\n",
      "Episode 26/100\n",
      "Average Reward (last 1 episodes): 21.00\n",
      "Current Exploration Rate (ε): 0.085\n",
      "--------------------\n",
      "Episode 27/100\n",
      "Average Reward (last 1 episodes): 14.00\n",
      "Current Exploration Rate (ε): 0.079\n",
      "--------------------\n",
      "Episode 27/100\n",
      "Average Reward (last 1 episodes): 14.00\n",
      "Current Exploration Rate (ε): 0.079\n",
      "--------------------\n",
      "Episode 28/100\n",
      "Average Reward (last 1 episodes): 15.00\n",
      "Current Exploration Rate (ε): 0.073\n",
      "--------------------\n",
      "Episode 28/100\n",
      "Average Reward (last 1 episodes): 15.00\n",
      "Current Exploration Rate (ε): 0.073\n",
      "--------------------\n",
      "Episode 29/100\n",
      "Average Reward (last 1 episodes): 10.00\n",
      "Current Exploration Rate (ε): 0.070\n",
      "--------------------\n",
      "Episode 29/100\n",
      "Average Reward (last 1 episodes): 10.00\n",
      "Current Exploration Rate (ε): 0.070\n",
      "--------------------\n",
      "Episode 30/100\n",
      "Average Reward (last 1 episodes): 16.00\n",
      "Current Exploration Rate (ε): 0.064\n",
      "--------------------\n",
      "Episode 30/100\n",
      "Average Reward (last 1 episodes): 16.00\n",
      "Current Exploration Rate (ε): 0.064\n",
      "--------------------\n",
      "Episode 31/100\n",
      "Average Reward (last 1 episodes): 21.00\n",
      "Current Exploration Rate (ε): 0.058\n",
      "--------------------\n",
      "Episode 31/100\n",
      "Average Reward (last 1 episodes): 21.00\n",
      "Current Exploration Rate (ε): 0.058\n",
      "--------------------\n",
      "Episode 32/100\n",
      "Average Reward (last 1 episodes): 10.00\n",
      "Current Exploration Rate (ε): 0.055\n",
      "--------------------\n",
      "Episode 32/100\n",
      "Average Reward (last 1 episodes): 10.00\n",
      "Current Exploration Rate (ε): 0.055\n",
      "--------------------\n",
      "Episode 33/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.051\n",
      "--------------------\n",
      "Episode 33/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.051\n",
      "--------------------\n",
      "Episode 34/100\n",
      "Average Reward (last 1 episodes): 18.00\n",
      "Current Exploration Rate (ε): 0.046\n",
      "--------------------\n",
      "Episode 34/100\n",
      "Average Reward (last 1 episodes): 18.00\n",
      "Current Exploration Rate (ε): 0.046\n",
      "--------------------\n",
      "Episode 35/100\n",
      "Average Reward (last 1 episodes): 9.00\n",
      "Current Exploration Rate (ε): 0.044\n",
      "--------------------\n",
      "Episode 35/100\n",
      "Average Reward (last 1 episodes): 9.00\n",
      "Current Exploration Rate (ε): 0.044\n",
      "--------------------\n",
      "Episode 36/100\n",
      "Average Reward (last 1 episodes): 8.00\n",
      "Current Exploration Rate (ε): 0.043\n",
      "--------------------\n",
      "Episode 36/100\n",
      "Average Reward (last 1 episodes): 8.00\n",
      "Current Exploration Rate (ε): 0.043\n",
      "--------------------\n",
      "Episode 37/100\n",
      "Average Reward (last 1 episodes): 96.00\n",
      "Current Exploration Rate (ε): 0.026\n",
      "--------------------\n",
      "Episode 37/100\n",
      "Average Reward (last 1 episodes): 96.00\n",
      "Current Exploration Rate (ε): 0.026\n",
      "--------------------\n",
      "Episode 38/100\n",
      "Average Reward (last 1 episodes): 30.00\n",
      "Current Exploration Rate (ε): 0.023\n",
      "--------------------\n",
      "Episode 38/100\n",
      "Average Reward (last 1 episodes): 30.00\n",
      "Current Exploration Rate (ε): 0.023\n",
      "--------------------\n",
      "Episode 39/100\n",
      "Average Reward (last 1 episodes): 25.00\n",
      "Current Exploration Rate (ε): 0.020\n",
      "--------------------\n",
      "Episode 39/100\n",
      "Average Reward (last 1 episodes): 25.00\n",
      "Current Exploration Rate (ε): 0.020\n",
      "--------------------\n",
      "Episode 40/100\n",
      "Average Reward (last 1 episodes): 29.00\n",
      "Current Exploration Rate (ε): 0.017\n",
      "--------------------\n",
      "Episode 40/100\n",
      "Average Reward (last 1 episodes): 29.00\n",
      "Current Exploration Rate (ε): 0.017\n",
      "--------------------\n",
      "Episode 41/100\n",
      "Average Reward (last 1 episodes): 22.00\n",
      "Current Exploration Rate (ε): 0.015\n",
      "--------------------\n",
      "Episode 41/100\n",
      "Average Reward (last 1 episodes): 22.00\n",
      "Current Exploration Rate (ε): 0.015\n",
      "--------------------\n",
      "Episode 42/100\n",
      "Average Reward (last 1 episodes): 78.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 42/100\n",
      "Average Reward (last 1 episodes): 78.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 43/100\n",
      "Average Reward (last 1 episodes): 36.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 43/100\n",
      "Average Reward (last 1 episodes): 36.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 44/100\n",
      "Average Reward (last 1 episodes): 9.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 44/100\n",
      "Average Reward (last 1 episodes): 9.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 45/100\n",
      "Average Reward (last 1 episodes): 25.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 45/100\n",
      "Average Reward (last 1 episodes): 25.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 46/100\n",
      "Average Reward (last 1 episodes): 12.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 46/100\n",
      "Average Reward (last 1 episodes): 12.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 47/100\n",
      "Average Reward (last 1 episodes): 30.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 47/100\n",
      "Average Reward (last 1 episodes): 30.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 48/100\n",
      "Average Reward (last 1 episodes): 18.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 48/100\n",
      "Average Reward (last 1 episodes): 18.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 49/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 49/100\n",
      "Average Reward (last 1 episodes): 17.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 50/100\n",
      "Average Reward (last 1 episodes): 53.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 50/100\n",
      "Average Reward (last 1 episodes): 53.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 51/100\n",
      "Average Reward (last 1 episodes): 9.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 51/100\n",
      "Average Reward (last 1 episodes): 9.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 52/100\n",
      "Average Reward (last 1 episodes): 169.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 52/100\n",
      "Average Reward (last 1 episodes): 169.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 53/100\n",
      "Average Reward (last 1 episodes): 114.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 53/100\n",
      "Average Reward (last 1 episodes): 114.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 54/100\n",
      "Average Reward (last 1 episodes): 150.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 54/100\n",
      "Average Reward (last 1 episodes): 150.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 55/100\n",
      "Average Reward (last 1 episodes): 62.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 55/100\n",
      "Average Reward (last 1 episodes): 62.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 56/100\n",
      "Average Reward (last 1 episodes): 161.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 56/100\n",
      "Average Reward (last 1 episodes): 161.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 57/100\n",
      "Average Reward (last 1 episodes): 145.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 57/100\n",
      "Average Reward (last 1 episodes): 145.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 58/100\n",
      "Average Reward (last 1 episodes): 89.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 58/100\n",
      "Average Reward (last 1 episodes): 89.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 59/100\n",
      "Average Reward (last 1 episodes): 43.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 59/100\n",
      "Average Reward (last 1 episodes): 43.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 60/100\n",
      "Average Reward (last 1 episodes): 179.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 60/100\n",
      "Average Reward (last 1 episodes): 179.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 61/100\n",
      "Average Reward (last 1 episodes): 49.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 61/100\n",
      "Average Reward (last 1 episodes): 49.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 62/100\n",
      "Average Reward (last 1 episodes): 297.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 62/100\n",
      "Average Reward (last 1 episodes): 297.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 63/100\n",
      "Average Reward (last 1 episodes): 465.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 63/100\n",
      "Average Reward (last 1 episodes): 465.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 64/100\n",
      "Average Reward (last 1 episodes): 184.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 64/100\n",
      "Average Reward (last 1 episodes): 184.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 65/100\n",
      "Average Reward (last 1 episodes): 152.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 65/100\n",
      "Average Reward (last 1 episodes): 152.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 66/100\n",
      "Average Reward (last 1 episodes): 92.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 66/100\n",
      "Average Reward (last 1 episodes): 92.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 67/100\n",
      "Average Reward (last 1 episodes): 68.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 67/100\n",
      "Average Reward (last 1 episodes): 68.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 68/100\n",
      "Average Reward (last 1 episodes): 74.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 68/100\n",
      "Average Reward (last 1 episodes): 74.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 69/100\n",
      "Average Reward (last 1 episodes): 113.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 69/100\n",
      "Average Reward (last 1 episodes): 113.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 70/100\n",
      "Average Reward (last 1 episodes): 67.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 70/100\n",
      "Average Reward (last 1 episodes): 67.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 71/100\n",
      "Average Reward (last 1 episodes): 143.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 71/100\n",
      "Average Reward (last 1 episodes): 143.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 72/100\n",
      "Average Reward (last 1 episodes): 112.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 72/100\n",
      "Average Reward (last 1 episodes): 112.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 73/100\n",
      "Average Reward (last 1 episodes): 89.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 73/100\n",
      "Average Reward (last 1 episodes): 89.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 74/100\n",
      "Average Reward (last 1 episodes): 147.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 74/100\n",
      "Average Reward (last 1 episodes): 147.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 75/100\n",
      "Average Reward (last 1 episodes): 29.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 75/100\n",
      "Average Reward (last 1 episodes): 29.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 76/100\n",
      "Average Reward (last 1 episodes): 185.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 76/100\n",
      "Average Reward (last 1 episodes): 185.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 77/100\n",
      "Average Reward (last 1 episodes): 168.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 77/100\n",
      "Average Reward (last 1 episodes): 168.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 78/100\n",
      "Average Reward (last 1 episodes): 128.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 78/100\n",
      "Average Reward (last 1 episodes): 128.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 79/100\n",
      "Average Reward (last 1 episodes): 12.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 79/100\n",
      "Average Reward (last 1 episodes): 12.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 80/100\n",
      "Average Reward (last 1 episodes): 46.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 80/100\n",
      "Average Reward (last 1 episodes): 46.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 81/100\n",
      "Average Reward (last 1 episodes): 119.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 81/100\n",
      "Average Reward (last 1 episodes): 119.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 82/100\n",
      "Average Reward (last 1 episodes): 129.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 82/100\n",
      "Average Reward (last 1 episodes): 129.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 83/100\n",
      "Average Reward (last 1 episodes): 27.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 83/100\n",
      "Average Reward (last 1 episodes): 27.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 84/100\n",
      "Average Reward (last 1 episodes): 114.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 84/100\n",
      "Average Reward (last 1 episodes): 114.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 85/100\n",
      "Average Reward (last 1 episodes): 153.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 85/100\n",
      "Average Reward (last 1 episodes): 153.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 86/100\n",
      "Average Reward (last 1 episodes): 80.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 86/100\n",
      "Average Reward (last 1 episodes): 80.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 87/100\n",
      "Average Reward (last 1 episodes): 45.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 87/100\n",
      "Average Reward (last 1 episodes): 45.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 88/100\n",
      "Average Reward (last 1 episodes): 109.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 88/100\n",
      "Average Reward (last 1 episodes): 109.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 89/100\n",
      "Average Reward (last 1 episodes): 147.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 89/100\n",
      "Average Reward (last 1 episodes): 147.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 90/100\n",
      "Average Reward (last 1 episodes): 164.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 90/100\n",
      "Average Reward (last 1 episodes): 164.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 91/100\n",
      "Average Reward (last 1 episodes): 36.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 91/100\n",
      "Average Reward (last 1 episodes): 36.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 92/100\n",
      "Average Reward (last 1 episodes): 175.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 92/100\n",
      "Average Reward (last 1 episodes): 175.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 93/100\n",
      "Average Reward (last 1 episodes): 195.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 93/100\n",
      "Average Reward (last 1 episodes): 195.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 94/100\n",
      "Average Reward (last 1 episodes): 151.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 94/100\n",
      "Average Reward (last 1 episodes): 151.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 95/100\n",
      "Average Reward (last 1 episodes): 82.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 95/100\n",
      "Average Reward (last 1 episodes): 82.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 96/100\n",
      "Average Reward (last 1 episodes): 232.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 96/100\n",
      "Average Reward (last 1 episodes): 232.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 97/100\n",
      "Average Reward (last 1 episodes): 144.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 97/100\n",
      "Average Reward (last 1 episodes): 144.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 98/100\n",
      "Average Reward (last 1 episodes): 79.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 98/100\n",
      "Average Reward (last 1 episodes): 79.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 99/100\n",
      "Average Reward (last 1 episodes): 106.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 99/100\n",
      "Average Reward (last 1 episodes): 106.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 100/100\n",
      "Average Reward (last 1 episodes): 21.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n",
      "Episode 100/100\n",
      "Average Reward (last 1 episodes): 21.00\n",
      "Current Exploration Rate (ε): 0.010\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Creates and sets up the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]  # 4 for CartPole\n",
    "action_size = env.action_space.n             # 2 for CartPole\n",
    "\n",
    "# Create the DQN agent\n",
    "agent = DQNAgent(state_size=state_size,\n",
    "                action_size=action_size,\n",
    "                learning_rate=0.01,     # Options to try: 0.001\n",
    "                discount_factor=0.95,\n",
    "                initial_epsilon=1.0,\n",
    "                epsilon_decay=0.995,\n",
    "                final_epsilon=0.01)\n",
    "\n",
    "# Training parameters\n",
    "n_episodes = 100\n",
    "target_update_frequency = 10  # Updates target network every 10 episodes\n",
    "print_interval = 1\n",
    "\n",
    "# Trains the agent\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = agent.train_episode(env)\n",
    "    \n",
    "    # Updates target network periodically\n",
    "    if episode % target_update_frequency == 0:\n",
    "        agent.update_target_model()\n",
    "    \n",
    "    # Prints progress\n",
    "    if (episode + 1) % print_interval == 0:\n",
    "        avg_reward = np.mean(agent.training_rewards[-print_interval:])\n",
    "        print(f\"Episode {episode + 1}/{n_episodes}\")\n",
    "        print(f\"Average Reward (last {print_interval} episodes): {avg_reward:.2f}\")\n",
    "        print(f\"Current Exploration Rate (ε): {agent.epsilon:.3f}\")\n",
    "        print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48288c1b",
   "metadata": {},
   "source": [
    "### Evaluating the Trained Q-Network Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28f5333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Episode 1: Reward = 100.0\n",
      "Evaluation Episode 2: Reward = 104.0\n",
      "Evaluation Episode 2: Reward = 104.0\n",
      "Evaluation Episode 3: Reward = 93.0\n",
      "Evaluation Episode 3: Reward = 93.0\n",
      "Evaluation Episode 4: Reward = 103.0\n",
      "Evaluation Episode 4: Reward = 103.0\n",
      "Evaluation Episode 5: Reward = 105.0\n",
      "\n",
      "Average Evaluation Reward: 101.00\n",
      "Evaluation Episode 5: Reward = 105.0\n",
      "\n",
      "Average Evaluation Reward: 101.00\n"
     ]
    }
   ],
   "source": [
    "# Creates environment for evaluation with rendering\n",
    "eval_env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# Runs evaluation episodes\n",
    "n_eval_episodes = 5\n",
    "eval_rewards = []\n",
    "\n",
    "for episode in range(n_eval_episodes):\n",
    "    state, _ = eval_env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Uses the learned policy (no exploration)\n",
    "        q_values = agent.model.predict(state, verbose=0)\n",
    "        action = np.argmax(q_values[0])\n",
    "        \n",
    "        # Takes action\n",
    "        next_state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Adds a small delay to better visualize the agent's behavior\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "    print(f\"Evaluation Episode {episode + 1}: Reward = {episode_reward}\")\n",
    "\n",
    "print(f\"\\nAverage Evaluation Reward: {np.mean(eval_rewards):.2f}\")\n",
    "\n",
    "# Close the evaluation environment\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b03f5",
   "metadata": {},
   "source": [
    "The above performance shows that Q-Network agent performed little better than the Q-Learning agent during evaluation. The performance of the Q-Network agent can further be improved over training with increased number of episodes and tuning its hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ce1b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
