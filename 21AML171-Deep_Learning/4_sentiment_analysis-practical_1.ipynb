{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974",
   "metadata": {
    "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974"
   },
   "source": [
    "# SENTIMENT ANALYSIS\n",
    "\n",
    "_**Consider IMDb reviews dataset and train two models â€“ one without any pretrained embeddings and other one with a contextualized pretrained embeddings, to classify sentiment of a movie review and then compare performance of these two models.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2a58a-c239-43af-9d91-60aed16baf46",
   "metadata": {
    "id": "1ca2a58a-c239-43af-9d91-60aed16baf46"
   },
   "source": [
    "**NOTES:** \n",
    "\n",
    "1. **Accelerated Hardware:** This notebook is advised to be executed with GPU to save time during model training. For relevant instructions and guidelines, please refer the README located at https://github.com/PradipKumarDas/Teaching/tree/master/21AML171-Deep_Learning.\n",
    "\n",
    "2. **Dependencies:** The following experiment was tested on TensorFlow 2.15.0. Later version of this packages was found to be default in Google Colaboratory and incompatible with this experiment as `tensorflow_hub.KerasLayer` was not not compatible as a layer in Keras sequence model. Hence, the following statement is suggested to be executed to install the spefied version of TensorFlow in the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b935e2f-496c-4728-b3f8-fdf59e68ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the installed version of TensorFlow in Google Colab.\n",
    "# \"pip\" can be replaced with \"conda\" for local computer with conda package manager.\n",
    "\n",
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFa9fYDtPF00",
   "metadata": {
    "id": "SFa9fYDtPF00"
   },
   "outputs": [],
   "source": [
    "# Installs specific version of TensorFlow in Google Colab.\n",
    "# \"pip\" can be replaced with \"conda\" for local computer with conda package manager.\n",
    "\n",
    "!pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723d6acf-753c-4cb2-9fc5-950f1b8cd01b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4964,
     "status": "ok",
     "timestamp": 1729585539393,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "723d6acf-753c-4cb2-9fc5-950f1b8cd01b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports required packages\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as tfhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203d40d-4280-4d00-ab6e-644bee487570",
   "metadata": {
    "id": "b203d40d-4280-4d00-ab6e-644bee487570"
   },
   "source": [
    "## Retrieval of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20be70-9b78-4c33-93c5-a33ac3cedb7c",
   "metadata": {
    "id": "3d20be70-9b78-4c33-93c5-a33ac3cedb7c"
   },
   "source": [
    "This experiment uses TensorFlow IMDb (Internet Movie Database) dataset containing English reviews for 50,000 movies - 25,000 for training and 25,000 for testing along with single binary target for each review indicating whether it is positive (1) or negative (0). Approximate download size is 80 megabytes (MB).\n",
    "\n",
    "The details of the dataset is available at https://www.tensorflow.org/datasets/catalog/imdb_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e164f7b-1da8-44b7-95ef-1fa942967cc4",
   "metadata": {
    "executionInfo": {
     "elapsed": 919,
     "status": "ok",
     "timestamp": 1729585545944,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "6e164f7b-1da8-44b7-95ef-1fa942967cc4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Following call may take several seconds to initiate downloading from the TensorFlow datasets.\n",
    "# The dowloading itself take few minutes to get complete.\n",
    "\n",
    "raw_train_set, raw_val_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "\n",
    "    # Splits dataset into train set of 22,500 [90%] instances,\n",
    "    # validation set of 2,500 [10%] instances and test set of 25,000 instances\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "\n",
    "    as_supervised=True  # Attaches targets with train, validation and test set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf35a50-fcb1-4358-b73a-7242a4161e88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1729584410207,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "9cf35a50-fcb1-4358-b73a-7242a4161e88",
    "outputId": "0aa1db9c-abec-4b19-b01e-d5ea85b02923",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "\n",
      "Label: 0\n",
      "\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "\n",
      "Label: 0\n",
      "\n",
      "\n",
      "Review: Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
      "\n",
      "Label: 0\n",
      "\n",
      "\n",
      "Review: This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n",
      "\n",
      "Label: 1\n",
      "\n",
      "\n",
      "Review: As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.\n",
      "\n",
      "Label: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Previews few of the reviews\n",
    "\n",
    "for review, label in raw_train_set.take(5):           # Takes first 5 reviews\n",
    "    print(\"Review:\", review.numpy().decode(\"utf-8\"))  # numpy().decode() converts string tensor into byte array first, then\n",
    "                                                      # the byte array to string\n",
    "    print(\"\\nLabel:\", label.numpy())                  # numpy() converts integer tensor to a scaler\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de0a625-36c5-4861-b252-0592fe1793b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1729585556515,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "0de0a625-36c5-4861-b252-0592fe1793b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffles and batches the train set over 32 instances per batch\n",
    "# For validation and test set, only batches are prepared as shuffling is not required in these sets\n",
    "# Prefetching overlaps the data preprocessing for step s+1 and while\n",
    "# the model performs training at step s to save time.\n",
    "\n",
    "tf.random.set_seed(42)  # Ensures reproducibility\n",
    "\n",
    "train_set = raw_train_set.shuffle(buffer_size=5000, seed=42).batch(32).prefetch(1)\n",
    "val_set = raw_val_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314522a-2a31-4761-8457-ba0df9ff3161",
   "metadata": {
    "id": "7314522a-2a31-4761-8457-ba0df9ff3161"
   },
   "source": [
    "## Preparation of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16deaf99-f4a0-4dec-be35-dc4d934502bb",
   "metadata": {
    "id": "16deaf99-f4a0-4dec-be35-dc4d934502bb"
   },
   "source": [
    "**Preprocessing layer to map text into integer sequences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4ad8a2-c2ff-4390-a236-be88070baafe",
   "metadata": {
    "executionInfo": {
     "elapsed": 5634,
     "status": "ok",
     "timestamp": 1729584423829,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "9a4ad8a2-c2ff-4390-a236-be88070baafe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performs tokenization (preprocessing) at the word level as reviews are in English\n",
    "\n",
    "# Limits vocabulary to 1000 tokens: 998 tokens for frequent words plus\n",
    "# a padding token and a token for unknown words\n",
    "vocabulary_size = 1000\n",
    "\n",
    "\n",
    "# Tokenizes the string data with TextVectorization layer\n",
    "\n",
    "text_vectorizer_layer = tf.keras.layers.TextVectorization(max_tokens=vocabulary_size)  # Initializes the layer\n",
    "\n",
    "text_vectorizer_layer.adapt(\n",
    "    train_set.map(lambda reviews, labels: reviews))  # Tokenizes the data by calling layer's adapt() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c69ffc-bf21-41ee-a058-202259f475f8",
   "metadata": {
    "id": "e4c69ffc-bf21-41ee-a058-202259f475f8"
   },
   "source": [
    "## Modeling with Recurrent Units & Trainable Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef65d2-3eca-4a74-ac1c-b8ca885cf298",
   "metadata": {
    "id": "e9ef65d2-3eca-4a74-ac1c-b8ca885cf298"
   },
   "source": [
    "**Creates the following sequential model and trains it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b5ddcf-7d72-4e49-abf1-b4ccb2908883",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217017,
     "status": "ok",
     "timestamp": 1729584654234,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "e4b5ddcf-7d72-4e49-abf1-b4ccb2908883",
    "outputId": "61cabc83-43e7-4c08-97fd-ac49ca8a5d4f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 49s 64ms/step - loss: 0.6934 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5012\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.6928 - accuracy: 0.5067 - val_loss: 0.6937 - val_accuracy: 0.4988\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 25s 36ms/step - loss: 0.6915 - accuracy: 0.5012 - val_loss: 0.6944 - val_accuracy: 0.4992\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 23s 32ms/step - loss: 0.6901 - accuracy: 0.5060 - val_loss: 0.6955 - val_accuracy: 0.5012\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 23s 33ms/step - loss: 0.6885 - accuracy: 0.5080 - val_loss: 0.6960 - val_accuracy: 0.5040\n"
     ]
    }
   ],
   "source": [
    "# Defines the size of the embedding\n",
    "embedding_size = 128\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Creates a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorizer_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_size),\n",
    "    tf.keras.layers.GRU(units=128, activation=\"tanh\", recurrent_activation=\"sigmoid\", return_sequences=False),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "# Compiles and fits the model\n",
    "\n",
    "model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=val_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd588c-6a58-40d2-b4e1-f65d1e36fe71",
   "metadata": {},
   "source": [
    "The above model fails to learn anything as the accuracy remains close to 50%. As `TextVectorization` layer pads shorter sequences with padding token (with ID 0) to make them as long as the longest sequence in the batch, the gated recurrent layer which is not good at remembering long sequences, when goes through the sequence of padding tokens, it forgets the review that was in the beginning of the sequence. That made the model perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959967d6-cc5f-4ccb-95b7-6101c5194f90",
   "metadata": {
    "id": "959967d6-cc5f-4ccb-95b7-6101c5194f90"
   },
   "source": [
    "## Modeling with Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509e6d8-5640-41b8-a6bc-63273eaf0945",
   "metadata": {
    "id": "0509e6d8-5640-41b8-a6bc-63273eaf0945"
   },
   "source": [
    "In the below mentioned technique called _masking_, the recurrnet layer is made aware of the padding tokens for it to ignore so that its prediction performance can improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102dde67-58ab-4f91-a5c3-b32a6691831f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218197,
     "status": "ok",
     "timestamp": 1729584889635,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "102dde67-58ab-4f91-a5c3-b32a6691831f",
    "outputId": "e89fd8e2-5bc9-4763-ad96-b32bf2ccb491",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 51s 65ms/step - loss: 0.5054 - accuracy: 0.7486 - val_loss: 0.4288 - val_accuracy: 0.8184\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 26s 37ms/step - loss: 0.3540 - accuracy: 0.8516 - val_loss: 0.3268 - val_accuracy: 0.8644\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.3003 - accuracy: 0.8760 - val_loss: 0.3386 - val_accuracy: 0.8532\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.2724 - accuracy: 0.8902 - val_loss: 0.3106 - val_accuracy: 0.8664\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 25s 35ms/step - loss: 0.2571 - accuracy: 0.8983 - val_loss: 0.3189 - val_accuracy: 0.8680\n"
     ]
    }
   ],
   "source": [
    "# Defines the size of the embedding\n",
    "embedding_size = 128\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Creates a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorizer_layer,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=vocabulary_size, \n",
    "        output_dim=embedding_size, \n",
    "        mask_zero=True),  # Masks input with ID=0 and propagates the info to lower layers for them to skip the padding tokens\n",
    "    tf.keras.layers.GRU(units=128, activation=\"tanh\", recurrent_activation=\"sigmoid\", return_sequences=False), \n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")])\n",
    "\n",
    "# Compiles and fits the model\n",
    "\n",
    "model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=val_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b84d8-89d7-4b12-af11-06ff2fd4311b",
   "metadata": {},
   "source": [
    "In the above model with masking, the accuracy on the validation set has reached around 86%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e648eb-7cff-4915-8355-be3d208063a0",
   "metadata": {
    "id": "91e648eb-7cff-4915-8355-be3d208063a0",
    "tags": []
   },
   "source": [
    "## Modeling with Pretrained Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9438c-4af7-443b-a277-aa302271ff86",
   "metadata": {
    "id": "62c9438c-4af7-443b-a277-aa302271ff86"
   },
   "source": [
    "The model's prediction performance could also be improved if a pretrained language model which is already trained over a large corpus is used and just gets fine-tuned on the task in hand. Amongst many, _Universal Sentence Encoder_ - the prerained language model from Google TensorFlow Hub is being considered here.\n",
    "\n",
    "But it is to be noted that due to the availability of only single commidity GPU over Google Colaboratory, only pretrained weights of the model was used. Having access to sufficient GPUs, pretrained weights can further be fine-tuned to get improved prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e252a0b-dcfc-4048-98c8-3c29b50af14c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 180257,
     "status": "ok",
     "timestamp": 1729586254433,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "7e252a0b-dcfc-4048-98c8-3c29b50af14c",
    "outputId": "89d32ba5-1eb8-4ac6-c779-e7d7d8f6e641",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "704/704 [==============================] - 32s 42ms/step - loss: 0.3816 - accuracy: 0.8392 - val_loss: 0.3294 - val_accuracy: 0.8496\n",
      "Epoch 2/5\n",
      "704/704 [==============================] - 30s 43ms/step - loss: 0.3256 - accuracy: 0.8606 - val_loss: 0.3269 - val_accuracy: 0.8504\n",
      "Epoch 3/5\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.3205 - accuracy: 0.8627 - val_loss: 0.3314 - val_accuracy: 0.8464\n",
      "Epoch 4/5\n",
      "704/704 [==============================] - 29s 41ms/step - loss: 0.3160 - accuracy: 0.8667 - val_loss: 0.3212 - val_accuracy: 0.8552\n",
      "Epoch 5/5\n",
      "704/704 [==============================] - 30s 43ms/step - loss: 0.3119 - accuracy: 0.8665 - val_loss: 0.3166 - val_accuracy: 0.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x77fd550d9930>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Serves the mention pretrained saved model (v4) as a keras layer\n",
    "    tfhub.KerasLayer(\n",
    "        handle=\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "        trainable=False,   # If set to True, it enables the pretrained model to be fine-tuned during training, but may take around one hour per epoch on a single commodity GPU \n",
    "        dtype=tf.string,                              # Expects a tf.string input tensor\n",
    "        input_shape=[]),                              # Expects a tensor of shape [batch_size] as input\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),     # Additional hidden layer to reduct output dimension before combining to output layer\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compiles and fits the model\n",
    "\n",
    "model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=val_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaddeb9-d4c1-495f-913c-4b1261bcb5f3",
   "metadata": {},
   "source": [
    "In the above model with (fixed-weight) pretrained language model, the accuracy on the validation set has reached around 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7c589-afe9-4cbc-840d-beecba02f8cf",
   "metadata": {
    "id": "7fc7c589-afe9-4cbc-840d-beecba02f8cf"
   },
   "source": [
    "**OBSERVATIONS:**\n",
    "1. Sentiment analysis was performed on a English movie reviews dataset.\n",
    "\n",
    "2. It was a supervised machine learning to predict viewer's (positive or negative) sentiment about a movie given its review provided by the viewer.\n",
    "\n",
    "3. TensorFlow was used for splitting, shuffling, batching and prefetching the data during training and prediction.\n",
    "\n",
    "4. Keras `TextVectorization` layer was used to tokenize each word in the reviews. Only 1000 mostly used words were used in the vocabulary.\n",
    "\n",
    "5. This experiment used three modeling approach as mentioned below.\n",
    "    - Modeling with trainable embedding layer and GRU layer that could learned nothing.\n",
    "    - Same modeling approach with masking achieved much better prediction performance due to the fact that the GRU could ignore padding token without processing them and forgetting about the far past review comment.\n",
    "    - Pretrained language model _Universal Sentence Encoder_ from TensorFlow Hub was used without fine-tuning its trained weights due to lack of powerful GPUs, and the same with fixed weights could achieve around 85% accuracy over validation set. Having access to specialized accelerators, its trained weights can be fine-tuned and the prediction performance is expected to be much better than what was achieved in this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H4S6N5gNPCCM",
   "metadata": {
    "id": "H4S6N5gNPCCM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
