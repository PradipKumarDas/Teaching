{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974",
   "metadata": {
    "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974"
   },
   "source": [
    "# SENTIMENT ANALYSIS USING RECURRENT NEURAL NETWORKS & EMBEDDINGS\n",
    "\n",
    "_**Building recurrent neural networks and a model with pretrained embeddings for sentiment analysis on IMDb dataset and comparing their performance.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723d6acf-753c-4cb2-9fc5-950f1b8cd01b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4964,
     "status": "ok",
     "timestamp": 1729585539393,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "723d6acf-753c-4cb2-9fc5-950f1b8cd01b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 15:59:26.527864: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-03 15:59:26.528668: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 15:59:26.532742: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 15:59:26.544207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740997766.564495  944224 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740997766.570225  944224 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-03 15:59:26.590199: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/pradip/anaconda3/envs/keras3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports required packages\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as tfhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203d40d-4280-4d00-ab6e-644bee487570",
   "metadata": {
    "id": "b203d40d-4280-4d00-ab6e-644bee487570"
   },
   "source": [
    "## Loading & Analyzing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20be70-9b78-4c33-93c5-a33ac3cedb7c",
   "metadata": {
    "id": "3d20be70-9b78-4c33-93c5-a33ac3cedb7c"
   },
   "source": [
    "This experiment uses TensorFlow IMDb (Internet Movie Database) dataset containing English reviews for 50,000 movies - 25,000 for training and 25,000 for testing along with single binary target for each review indicating whether it is positive (1) or negative (0). Approximate download size is 80 megabytes (MB). The details of the dataset is available at https://www.tensorflow.org/datasets/catalog/imdb_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e164f7b-1da8-44b7-95ef-1fa942967cc4",
   "metadata": {
    "executionInfo": {
     "elapsed": 919,
     "status": "ok",
     "timestamp": 1729585545944,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "6e164f7b-1da8-44b7-95ef-1fa942967cc4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 15:59:30.769941: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Following call may take several seconds to initiate downloading from the TensorFlow datasets.\n",
    "# The dowloading itself take few minutes to get complete.\n",
    "\n",
    "(train_set_raw, val_set_raw, test_set_raw), ds_info = tfds.load(\n",
    "    name=\"imdb_reviews\",                                # Name of the dataset\n",
    "    # Splits dataset into train set of 22,500 [90%] instances,\n",
    "    # validation set of 2,500 [10%] instances and test set of 25,000 instances\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],       # Split of the data to load\n",
    "    as_supervised=True,                                 # To attach labels with each split\n",
    "    with_info=True                                      # Also to return dataset information\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066332d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    full_name='imdb_reviews/plain_text/1.0.0',\n",
      "    description=\"\"\"\n",
      "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
      "    classification containing substantially more data than previous benchmark\n",
      "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
      "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    Plain text\n",
      "    \"\"\",\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    data_path='/home/pradip/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=80.23 MiB,\n",
      "    dataset_size=129.83 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('text', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
      "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# [OPTIONAL] Prints information related to train set\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf35a50-fcb1-4358-b73a-7242a4161e88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1729584410207,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "9cf35a50-fcb1-4358-b73a-7242a4161e88",
    "outputId": "0aa1db9c-abec-4b19-b01e-d5ea85b02923",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "\n",
      "Label: 0\n",
      "\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "\n",
      "Label: 0\n",
      "\n",
      "\n",
      "Review: Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
      "\n",
      "Label: 0\n",
      "\n",
      "\n",
      "Review: This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n",
      "\n",
      "Label: 1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 15:59:31.033947: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-03-03 15:59:31.052671: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-03-03 15:59:31.053147: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# [OPTIONAL] Previews few of the reviews\n",
    "\n",
    "for review, label in train_set_raw.take(4):           # Takes first 5 reviews\n",
    "    print(\"Review:\", review.numpy().decode(\"utf-8\"))  # numpy().decode() converts string tensor into byte array first, then\n",
    "                                                      # the byte array to string\n",
    "    print(\"\\nLabel:\", label.numpy())                  # numpy() converts integer tensor to a scaler\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314522a-2a31-4761-8457-ba0df9ff3161",
   "metadata": {
    "id": "7314522a-2a31-4761-8457-ba0df9ff3161"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0de0a625-36c5-4861-b252-0592fe1793b6",
   "metadata": {
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1729585556515,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "0de0a625-36c5-4861-b252-0592fe1793b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prefetching overlaps the data preprocessing for step s+1 and while\n",
    "# the model performs training at step s to save time.\n",
    "\n",
    "# Sets the global random seed for prerations that rely on a random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Sets the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Shuffles instances and enables prefetching for effective batch processing\n",
    "# Shuffling is applicable only for train set\n",
    "train_set = train_set_raw.shuffle(buffer_size=5000, seed=42).batch(32).prefetch(1)\n",
    "val_set = val_set_raw.batch(32).prefetch(1)\n",
    "test_set = test_set_raw.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c69ffc-bf21-41ee-a058-202259f475f8",
   "metadata": {
    "id": "e4c69ffc-bf21-41ee-a058-202259f475f8"
   },
   "source": [
    "## Modeling\n",
    "_First models with recurrent units and training word embedding without masking and then models with the same approach but with masking enabled._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16deaf99-f4a0-4dec-be35-dc4d934502bb",
   "metadata": {
    "id": "16deaf99-f4a0-4dec-be35-dc4d934502bb"
   },
   "source": [
    "**Tokenizing Text**\n",
    "\n",
    "Considering language of the text as English, it prepares a tokenizer to tokenize text at the word level and to use the tokenizer in both the RNN based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4ad8a2-c2ff-4390-a236-be88070baafe",
   "metadata": {
    "executionInfo": {
     "elapsed": 5634,
     "status": "ok",
     "timestamp": 1729584423829,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "9a4ad8a2-c2ff-4390-a236-be88070baafe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 15:59:34.959520: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Limits vocabulary to 1000 words: 998 tokens for frequent words plus\n",
    "# one token for padding and one for unknown words\n",
    "vocabulary_size = 1000\n",
    "\n",
    "# Initializes TextVectorization layer to tokenize the input text\n",
    "text_vectorizer_layer = tf.keras.layers.TextVectorization(max_tokens=vocabulary_size)\n",
    "\n",
    "# Passes train set for that layer to adapt for it so that it can tokenize the \n",
    "# input text during training and inferencing\n",
    "text_vectorizer_layer.adapt(\n",
    "    train_set.map(lambda reviews, labels: reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8b3ce",
   "metadata": {},
   "source": [
    "### Modeling with Trainable Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b5ddcf-7d72-4e49-abf1-b4ccb2908883",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217017,
     "status": "ok",
     "timestamp": 1729584654234,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "e4b5ddcf-7d72-4e49-abf1-b4ccb2908883",
    "outputId": "61cabc83-43e7-4c08-97fd-ac49ca8a5d4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defines the size of the embedding \n",
    "embedding_size = 128\n",
    "\n",
    "tf.random.set_seed(42)        # Sets the global random seed for operations that rely on a random seed\n",
    "\n",
    "# Creates a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorizer_layer,          # Vectorizer [already adapted ealier] to tokenize input string\n",
    "\n",
    "    # Trainable embedding that learns to represent each token into a dense vector of fixed size\n",
    "    tf.keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_size),\n",
    "\n",
    "    # Recurrent unit as a layer\n",
    "    tf.keras.layers.GRU(\n",
    "        units=128,                  # Number of outputs\n",
    "        return_sequences=False),    # Returns the full sequence instead of last output in output sequence     \n",
    "\n",
    "    # Regular dense layer with no. of outputs and activation function \n",
    "    # appropriate for binary classification task\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f77d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 278ms/step - accuracy: 0.4948 - loss: 0.6939 - val_accuracy: 0.5008 - val_loss: 0.6929\n",
      "Epoch 2/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 280ms/step - accuracy: 0.5004 - loss: 0.6929 - val_accuracy: 0.5032 - val_loss: 0.6928\n",
      "Epoch 3/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 289ms/step - accuracy: 0.5075 - loss: 0.6921 - val_accuracy: 0.5020 - val_loss: 0.6936\n",
      "Epoch 4/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 285ms/step - accuracy: 0.5035 - loss: 0.6908 - val_accuracy: 0.5012 - val_loss: 0.6965\n",
      "Epoch 5/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 288ms/step - accuracy: 0.5078 - loss: 0.6885 - val_accuracy: 0.5008 - val_loss: 0.6975\n"
     ]
    }
   ],
   "source": [
    "# Compiles and fits the model\n",
    "\n",
    "model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=val_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd588c-6a58-40d2-b4e1-f65d1e36fe71",
   "metadata": {},
   "source": [
    "**Observation:** The above model fails to learn anything as the accuracy remains close to 50%. As `TextVectorization` layer pads shorter sequences with padding token (with ID 0) to make them as long as the longest sequence in the batch, the gated recurrent layer which is not good at remembering long sequences, when goes through the sequence of padding tokens, forgets the review that was in the beginning of the sequence. That made the model perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959967d6-cc5f-4ccb-95b7-6101c5194f90",
   "metadata": {
    "id": "959967d6-cc5f-4ccb-95b7-6101c5194f90"
   },
   "source": [
    "### Modeling with Masking\n",
    "_Masking is enabled at the embedding layer for it to propagate this information to all downstream layers to skip the padding tokens so that prediction performance of the model improves._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d8f9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorizer_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=vocabulary_size, \n",
    "                              output_dim=embedding_size, \n",
    "                              mask_zero=True),              # Masks padding tokens (whose ID is 0)\n",
    "    tf.keras.layers.GRU(units=128, return_sequences=False), \n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102dde67-58ab-4f91-a5c3-b32a6691831f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218197,
     "status": "ok",
     "timestamp": 1729584889635,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "102dde67-58ab-4f91-a5c3-b32a6691831f",
    "outputId": "e89fd8e2-5bc9-4763-ad96-b32bf2ccb491",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 16:16:20.134056: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 314ms/step - accuracy: 0.6548 - loss: 0.5946 - val_accuracy: 0.8236 - val_loss: 0.4118\n",
      "Epoch 2/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 317ms/step - accuracy: 0.8592 - loss: 0.3371 - val_accuracy: 0.8660 - val_loss: 0.3174\n",
      "Epoch 3/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 317ms/step - accuracy: 0.8844 - loss: 0.2809 - val_accuracy: 0.8680 - val_loss: 0.3051\n",
      "Epoch 4/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 319ms/step - accuracy: 0.8940 - loss: 0.2610 - val_accuracy: 0.8640 - val_loss: 0.3135\n",
      "Epoch 5/5\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 318ms/step - accuracy: 0.9043 - loss: 0.2380 - val_accuracy: 0.8536 - val_loss: 0.3253\n"
     ]
    }
   ],
   "source": [
    "# Compiles and fits the model\n",
    "\n",
    "model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=val_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b84d8-89d7-4b12-af11-06ff2fd4311b",
   "metadata": {},
   "source": [
    "In the above model with masking, the accuracy on the validation set has reached around 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "813f097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 96ms/step - accuracy: 0.8630 - loss: 0.3209\n",
      "Test Performance [Accuracy]: 86.4%\n"
     ]
    }
   ],
   "source": [
    "# Evaluates model over test set\n",
    "model_test_performance = model.evaluate(test_set)\n",
    "\n",
    "print(\"Test Performance [Accuracy]: {:.1f}%\".format(model_test_performance[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e648eb-7cff-4915-8355-be3d208063a0",
   "metadata": {
    "id": "91e648eb-7cff-4915-8355-be3d208063a0",
    "tags": []
   },
   "source": [
    "### Modeling with Pretrained Embeddings\n",
    "_Experimenting with pretrained sentence-level embeddings for classification task._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9438c-4af7-443b-a277-aa302271ff86",
   "metadata": {
    "id": "62c9438c-4af7-443b-a277-aa302271ff86"
   },
   "source": [
    "One of the pretrained sentense encoders called _Universal Sentence Encoder_ (USE) from TensorFlow Hub was used. It is already trained over a large corpus and helps in finding sentence-level meaning similarity enenbling better performance for downstream classification tasks. The model could also be fine-tuned over the task in hand to improve prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "463a3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"    # Location of USE to download from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d740b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class USE_Embedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Defines a custom layer to wrap USE. This is primarily due to the fact that newer TensorFlow \n",
    "    frameworks [version 2.15+] do not anymore support saved model to be used as layer in Keras sequential model.\n",
    "    \"\"\"\n",
    "    def __init__(self, module_url, **kwargs):           # Used as constructor\n",
    "        super(USE_Embedding, self).__init__(**kwargs)   # Calls initialization method of the parent class\n",
    "\n",
    "        # Class variable 'embed' is set to embedding layer\n",
    "        # NOTE: Downloading the below module which is around 1 GB in size may take few minutes to complete\n",
    "        self.embed = tfhub.KerasLayer(module_url, trainable=True, input_shape=[], dtype=tf.string)\n",
    "\n",
    "    def call(self, inputs):                             # Gets called when an instance behaves like a function\n",
    "        return self.embed(inputs)                       # Returns the embeddings against the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ac4ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a functional model containing USE as a layer\n",
    "\n",
    "input = tf.keras.Input(shape=(), dtype=tf.string)               # shape=() indicates input sequence length is unknown or may vary\n",
    "\n",
    "embedding = USE_Embedding(module_url)(input)                    # Pretrained embedding layer to receive input\n",
    "\n",
    "dense = tf.keras.layers.Dense(64, activation=\"relu\")(embedding) # Regular dense layer\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)  # Regular one-output dense layer for binary classification\n",
    "\n",
    "model = tf.keras.Model(inputs=input, outputs=output)            # Groups layers as a model for training and/or inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72ba0db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ use__embedding_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">USE_Embedding</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ use__embedding_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mUSE_Embedding\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,897</span> (128.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,897\u001b[0m (128.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,897</span> (128.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,897\u001b[0m (128.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [OPTIONAL] Shows the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "753aaee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 31ms/step - accuracy: 0.7967 - loss: 0.4675 - val_accuracy: 0.8504 - val_loss: 0.3283\n",
      "Epoch 2/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.8605 - loss: 0.3290 - val_accuracy: 0.8548 - val_loss: 0.3232\n",
      "Epoch 3/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.8628 - loss: 0.3197 - val_accuracy: 0.8496 - val_loss: 0.3250\n",
      "Epoch 4/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8683 - loss: 0.3183 - val_accuracy: 0.8520 - val_loss: 0.3222\n",
      "Epoch 5/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8680 - loss: 0.3125 - val_accuracy: 0.8564 - val_loss: 0.3200\n",
      "Epoch 6/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8674 - loss: 0.3120 - val_accuracy: 0.8520 - val_loss: 0.3168\n",
      "Epoch 7/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8727 - loss: 0.3075 - val_accuracy: 0.8580 - val_loss: 0.3164\n",
      "Epoch 8/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.8734 - loss: 0.3010 - val_accuracy: 0.8592 - val_loss: 0.3152\n",
      "Epoch 9/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8760 - loss: 0.2944 - val_accuracy: 0.8592 - val_loss: 0.3161\n",
      "Epoch 10/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8798 - loss: 0.2928 - val_accuracy: 0.8604 - val_loss: 0.3128\n"
     ]
    }
   ],
   "source": [
    "# Compiles the mode and trains it\n",
    "\n",
    "model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_set, \n",
    "                    validation_data=val_set, \n",
    "                    epochs=10, \n",
    "                    callbacks=[\n",
    "                        tf.keras.callbacks.ModelCheckpoint(\n",
    "                            \"./model_weights/my_universal_sentence_encoder_model.weights.h5\", \n",
    "                            monitor='val_accuracy', \n",
    "                            save_best_only=True, \n",
    "                            save_weights_only=True)\n",
    "                        # tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', restore_best_weights=True)]\n",
    "                    ])\n",
    "\n",
    "# Loads back the weights of the best model\n",
    "model.load_weights(\"./model_weights/my_universal_sentence_encoder_model.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaddeb9-d4c1-495f-913c-4b1261bcb5f3",
   "metadata": {},
   "source": [
    "The validation accuracy of the above model has reached to 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "170cd32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.8605 - loss: 0.3230\n",
      "Test Performance [Accuracy]: 86.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluates model over test set\n",
    "model_test_performance = model.evaluate(test_set)\n",
    "\n",
    "print(\"Test Performance [Accuracy]: {:.1f}%\".format(model_test_performance[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7c589-afe9-4cbc-840d-beecba02f8cf",
   "metadata": {
    "id": "7fc7c589-afe9-4cbc-840d-beecba02f8cf"
   },
   "source": [
    "**Observations**\n",
    "\n",
    "This experiment used three modeling approaches as mentioned below.\n",
    "\n",
    "1. Modeling with trainable embedding layer and GRU layer that could learned nothing.\n",
    "\n",
    "2. Same modeling approach with masking achieved much better prediction performance due to the fact that the GRU could ignore padding token without processing them and it helped it for not to forget about the far past reviews.\n",
    "\n",
    "3. Pretrained language model _Universal Sentence Encoder_ from TensorFlow Hub could achieve around 85% accuracy over validation set. Having access to specialized accelerators, its trained weights can be fine-tuned and the prediction performance is expected to be much better than what was achieved in this experiment.\n",
    "\n",
    "**Known Issues**\n",
    "\n",
    "1. One of the concerns of this experiment is that the pretrained embedding is non-tunable [refer model summary to find that the trainable parameters is shown as zero] even though the layer that wraps the embedding was set as trainable [refer wrapper class `USE_Embedding`]. Fixing that problem could improve the prediction performance (accuracy) of the model.\n",
    "\n",
    "2. While training the USE pretrained embeddings based model in Google Colab on T4 GPU with TensorFlow version 2.18.0, XLA compiler that accelerates computations on GPU raised TensorFlow graph execution error. Since the error message highlighted \"T=DT_STRING\" it is highly likely that the string input to the USE embedding layer is causing the problem. The XLA compiler might not have the necessary kernels to handle string data on the GPU. Fixing this problem could further improve model training time performance.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "keras3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
