# Additional Resources on the PyTorch Workflow

## End-to-End Machine Learning Cycle

- [Full Stack Deep Learning: News, community, and courses for people building AI-powered products](https://fullstackdeeplearning.com/) - Brings people together to learn and share best practices across the entire lifecycle of an AI-powered product - from defining the problem and picking a GPU or foundation model to production deployment and continual learning to user experience design.

- [Made With ML by AnyScale](https://madewithml.com/) - An MLOps course by Goku Mohandas teaching how to combine machine learning with software engineering to design, develop, deploy and iterate on end-to-end production machine learning applications. 

## Loss Functions, Optimizers & Gradients

- [An Overview of Gradient Descent Optimization Algorithms](https://www.ruder.io/optimizing-gradient-descent) - Sebastian Ruder provides an overview of gradient descent optimization algorithms such as momentum, AdaGrad, RMSProp and Adam.

## Computational Graphs & Autograd

- [micrograd](https://github.com/karpathy/micrograd) -   A tiny scalar-valued autograd engine and a neural net library developed by Andrej Karpathy | [YouTube](https://www.youtube.com/watch?v=VMj-3S1tku0)

- [How the backpropagation algorithm works](https://neuralnetworksanddeeplearning.com/chap2.html) â€” Michael Nielsen discusses how to compute the gradient of the cost function by a fast algorithm known as backpropagation.

- [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/pdf/1802.01528) - A paper by Terence Parr and Jeremy Howard where they explained all the matrix calculus needed to understand the training of deep neural networks.
