{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974",
   "metadata": {
    "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974"
   },
   "source": [
    "# SENTIMENT ANALYSIS USING RECURRENT NEURAL NETWORKS & EMBEDDINGS\n",
    "\n",
    "_**Building recurrent neural networks and a model with pretrained embeddings for sentiment analysis on IMDb dataset and comparing their performance.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d6acf-753c-4cb2-9fc5-950f1b8cd01b",
   "metadata": {
    "executionInfo": {
     "elapsed": 4964,
     "status": "ok",
     "timestamp": 1729585539393,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "723d6acf-753c-4cb2-9fc5-950f1b8cd01b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports required packages\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# import tensorflow_hub as tfhub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b203d40d-4280-4d00-ab6e-644bee487570",
   "metadata": {
    "id": "b203d40d-4280-4d00-ab6e-644bee487570"
   },
   "source": [
    "## Data Acquisition & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20be70-9b78-4c33-93c5-a33ac3cedb7c",
   "metadata": {
    "id": "3d20be70-9b78-4c33-93c5-a33ac3cedb7c"
   },
   "source": [
    "This experiment uses a large movie review dataset called IMDb dataset (https://ai.stanford.edu/~amaas/data/sentiment/) that contains English reviews for 50,000 movies - 25,000 for training and 25,000 for testing along with single binary target for each review indicating whether it is positive (1) or negative (0). Approximate download size is 80 megabytes (MB). The details of the dataset is available at https://www.tensorflow.org/datasets/catalog/imdb_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e44e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./../data/imdb_reviews_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir, \"train\"),     # Path to train directory to load positive and negative samples\n",
    "    labels= ...,            # Set it to \"inferred\" to infer labels from the name of the directories\n",
    "    label_mode= ...,        # Set to \"binary\" to indicate it a binary classfication task\n",
    "    class_names= ...,       # Set to a list with values \"neg\" and \"pos\" to read these subdirectories from and not from non-labeled \"unsup\"\n",
    "    batch_size= ...,        # Set batch size to 32 - the default size\n",
    "    shuffle= ...,           # Set it to `True` to shuffle the samples as part of best practice for model training\n",
    "    seed= ...,              # Set to any integer such as 42 to enable reproducibility for the sequences of samples across classes and splits\n",
    "    validation_split= ...,  # Set to to 0.2 as an indicator to seperate 20% of the training samples to be used as validation set\n",
    "    subset= ...             # Set it to \"both\" for both train and validation dataset to be return to work with in subsequent steps\n",
    "    )\n",
    "\n",
    "# Pass the same arguments to the respective parameters for test set, too\n",
    "test_set = tf.keras.utils.text_dataset_from_directory(\n",
    "    os.path.join(dataset_dir, \"test\"), \n",
    "    labels= ...,\n",
    "    label_mode= ...,\n",
    "    class_names= ...,\n",
    "    batch_size= ...,\n",
    "    seed= ...,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass `tf.data.AUTOTUNE` as argument to `prefetch` methods below to parallelize data transformation. This will help \n",
    "# overlapping the data preprocessing for step s+1 and while the model performs training at step s to save time.\n",
    "\n",
    "train_set = train_set.prefetch( ... )\n",
    "val_set = val_set.prefetch( ... )\n",
    "test_set = test_set.prefetch( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cc2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads few of samples by taking them randomly from the train set\n",
    "\n",
    "for reviews, labels in train_set.take(1):\n",
    "    for i in range(0, 5):\n",
    "        print(f\"Review: {reviews[i].numpy().decode(\"utf-8\")}\\nLabel: {labels[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c69ffc-bf21-41ee-a058-202259f475f8",
   "metadata": {
    "id": "e4c69ffc-bf21-41ee-a058-202259f475f8"
   },
   "source": [
    "## Modeling\n",
    "_First models with training embedding - both with and without masking and then models with pretrained embeddings._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16deaf99-f4a0-4dec-be35-dc4d934502bb",
   "metadata": {
    "id": "16deaf99-f4a0-4dec-be35-dc4d934502bb"
   },
   "source": [
    "**Creating a Tokenizer**\n",
    "\n",
    " It prepares a tokenizer to tokenize text at the word level. It consideres English as text language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ad8a2-c2ff-4390-a236-be88070baafe",
   "metadata": {
    "executionInfo": {
     "elapsed": 5634,
     "status": "ok",
     "timestamp": 1729584423829,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "9a4ad8a2-c2ff-4390-a236-be88070baafe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limits vocabulary to 1000 words: 998 tokens for frequent words plus\n",
    "# one token for padding and one for unknown words\n",
    "vocabulary_size = 1000\n",
    "\n",
    "# Initialize TextVectorization layer to tokenize the input text by passing the following arguments\n",
    "text_vectorizer_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens= ...,    # Set to the size of vocabulary. Effective number of tokens is (max_tokens - 1 - (1 if output_mode == \"int\" else 0))\n",
    "    standardize= ...,   # Set it to 'lower_and_strip_punctuation' to lower case input string and to remove all punctuations\n",
    "    split= ...,         # Set it to \"whitespace\" to split input text on\n",
    "    output_mode= ...,   # Set it to \"int\" to output integer indices - one integer index per split string token. (0 is reserved for masked locations)\n",
    "    )\n",
    "\n",
    "# The text vectorizer gets adapted by passing train set through it so that\n",
    "# it can tokenize the input text during training and inferencing\n",
    "text_vectorizer_layer.adapt(\n",
    "    train_set.map(lambda reviews, labels: reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089eae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passes a sample text to check if text vectorizer works by observing \n",
    "# the output integer indices corresponding to the words in the input string\n",
    "text_vectorizer_layer(\"This is a sample positive movie review.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8b3ce",
   "metadata": {},
   "source": [
    "### Modeling with Trainable Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c773414",
   "metadata": {},
   "source": [
    "#### Modeling without Masking\n",
    "`TextVectorization` layer pads shorter sequences with padding token (with ID 0) to make them as long as the longest sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5ddcf-7d72-4e49-abf1-b4ccb2908883",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217017,
     "status": "ok",
     "timestamp": 1729584654234,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "e4b5ddcf-7d72-4e49-abf1-b4ccb2908883",
    "outputId": "61cabc83-43e7-4c08-97fd-ac49ca8a5d4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defines the size of the embedding \n",
    "embedding_size = 128\n",
    "\n",
    "tf.random.set_seed(42)        # Sets the global random seed for operations that rely on a random seed\n",
    "\n",
    "# Create a sequential model with the specific layers as mentioned\n",
    "model = tf.keras.Sequential([\n",
    "    ...,    # Set to `text_vectorizer_layer` as a Vectorizer layer (already adapted) to first tokenize input string\n",
    "\n",
    "    ...,    # An `tf.keras.layers.Embedding` layer (with parameter `input_dim` set to vocabulary size\n",
    "            # and `output_dim` set to embedding size) as trainable embedding that learns to represent\n",
    "            # each token into a dense vector of fixed size\n",
    "\n",
    "    ...,    # A `tf.keras.layers.GRU` layer (with parameter `unit` set to 128 as output units and \n",
    "            # `return_sequences` set to `False` to return last output in output sequence (instead of full sequence)\n",
    "\n",
    "    ...     # Regular dense layer `tf.keras.layers.Dense` with parameter 1 as output unit (for binary classification)\n",
    "            # and `activation` set to \"sigmoid\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with \"nadam\" as optimizer, \"binary_crossentropy\" as loss function and \n",
    "# [\"accuracy\"] as list of metrics\n",
    "model.compile(optimizer= ..., loss= ..., metrics= ...)\n",
    "\n",
    "# Fit the model by passing train set in the first parameter, validation set and 5 as number of epochs in\n",
    "# the remaining two named parameters\n",
    "history = model.fit( ..., validation_data= ..., epochs= ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd588c-6a58-40d2-b4e1-f65d1e36fe71",
   "metadata": {},
   "source": [
    "Observe the model's prediction performance on the validation set and analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959967d6-cc5f-4ccb-95b7-6101c5194f90",
   "metadata": {
    "id": "959967d6-cc5f-4ccb-95b7-6101c5194f90"
   },
   "source": [
    "#### Modeling with Masking\n",
    "_Masking is enabled at the embedding layer for it to propagate this information to all downstream layers to skip the padding tokens so that prediction performance of the model improves._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f9eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another sequential model as instructed above, but this time pass argument `True` to parameter\n",
    "# `mask_zero` to function `tf.keras.layers.Embedding` to masks padding tokens (whose ID is 0)\n",
    "model = tf.keras.Sequential([\n",
    "    ...,\n",
    "    ...,\n",
    "    ...,\n",
    "    ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dde67-58ab-4f91-a5c3-b32a6691831f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218197,
     "status": "ok",
     "timestamp": 1729584889635,
     "user": {
      "displayName": "Pradip Kumar Das",
      "userId": "12916671935047684031"
     },
     "user_tz": -330
    },
    "id": "102dde67-58ab-4f91-a5c3-b32a6691831f",
    "outputId": "e89fd8e2-5bc9-4763-ad96-b32bf2ccb491",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the same settings to compile and fit the model once again\n",
    "\n",
    "model.compile(optimizer= ..., loss= ..., metrics= ...)\n",
    "\n",
    "history = model.fit( ..., validation_data= ..., epochs=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b84d8-89d7-4b12-af11-06ff2fd4311b",
   "metadata": {},
   "source": [
    "Observe the model's prediction performance on the validation set post masking and analyze it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model over test set\n",
    "model_test_performance = model.evaluate(test_set)\n",
    "\n",
    "print(f\"Test Performance (accuracy): {model_test_performance[1] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e648eb-7cff-4915-8355-be3d208063a0",
   "metadata": {
    "id": "91e648eb-7cff-4915-8355-be3d208063a0",
    "tags": []
   },
   "source": [
    "### [OPTIONAL] Modeling with Pretrained Embeddings\n",
    "_Experimenting with pretrained sentence-level embeddings for classification task._\n",
    "\n",
    "**Known Issues**\n",
    "\n",
    "1. One of the concerns of this experiment is that the pretrained embedding is non-tunable [refer model summary to find that the trainable parameters is shown as zero] even though the layer that wraps the embedding was set as trainable [refer wrapper class `USE_Embedding`]. Fixing that problem could improve the prediction performance (accuracy) of the model.\n",
    "\n",
    "2. While training the USE pretrained embeddings based model in Google Colab on T4 GPU with TensorFlow version 2.18.0, XLA compiler that accelerates computations on GPU raised TensorFlow graph execution error. Since the error message highlighted \"T=DT_STRING\" it is highly likely that the string input to the USE embedding layer is causing the problem. The XLA compiler might not have the necessary kernels to handle string data on the GPU. Fixing this problem could further improve model training time performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9438c-4af7-443b-a277-aa302271ff86",
   "metadata": {
    "id": "62c9438c-4af7-443b-a277-aa302271ff86"
   },
   "source": [
    "One of the pretrained sentense encoders called _Universal Sentence Encoder_ (USE) from TensorFlow Hub was used. It is already trained over a large corpus and helps in finding sentence-level meaning similarity enenbling better performance for downstream classification tasks. The model could also be fine-tuned over the task in hand to improve prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"    # Location of USE to download from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class USE_Embedding(tf.keras.layers.Layer):\n",
    "#     \"\"\"\n",
    "#     Defines a custom layer to wrap USE. This is primarily due to the fact that newer TensorFlow \n",
    "#     frameworks [version 2.15+] do not anymore support saved model to be used as layer in Keras sequential model.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, module_url, **kwargs):           # Used as constructor\n",
    "#         super(USE_Embedding, self).__init__(**kwargs)   # Calls initialization method of the parent class\n",
    "\n",
    "#         # Class variable 'embed' is set to embedding layer\n",
    "#         # NOTE: Downloading the below module which is around 1 GB in size may take few minutes to complete\n",
    "#         self.embed = tfhub.KerasLayer(module_url, trainable=True, input_shape=[], dtype=tf.string)\n",
    "\n",
    "#     def call(self, inputs):                             # Gets called when an instance behaves like a function\n",
    "#         return self.embed(inputs)                       # Returns the embeddings against the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates a functional model containing USE as a layer\n",
    "\n",
    "# input = tf.keras.Input(shape=(), dtype=tf.string)               # shape=() indicates input sequence length is unknown or may vary\n",
    "\n",
    "# embedding = USE_Embedding(module_url)(input)                    # Pretrained embedding layer to receive input\n",
    "\n",
    "# dense = tf.keras.layers.Dense(64, activation=\"relu\")(embedding) # Regular dense layer\n",
    "\n",
    "# output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)  # Regular one-output dense layer for binary classification\n",
    "\n",
    "# model = tf.keras.Model(inputs=input, outputs=output)            # Groups layers as a model for training and/or inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba0db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ use__embedding_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">USE_Embedding</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ use__embedding_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mUSE_Embedding\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,897</span> (128.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,897\u001b[0m (128.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,897</span> (128.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,897\u001b[0m (128.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shows the model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753aaee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 31ms/step - accuracy: 0.7967 - loss: 0.4675 - val_accuracy: 0.8504 - val_loss: 0.3283\n",
      "Epoch 2/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.8605 - loss: 0.3290 - val_accuracy: 0.8548 - val_loss: 0.3232\n",
      "Epoch 3/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.8628 - loss: 0.3197 - val_accuracy: 0.8496 - val_loss: 0.3250\n",
      "Epoch 4/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8683 - loss: 0.3183 - val_accuracy: 0.8520 - val_loss: 0.3222\n",
      "Epoch 5/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8680 - loss: 0.3125 - val_accuracy: 0.8564 - val_loss: 0.3200\n",
      "Epoch 6/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8674 - loss: 0.3120 - val_accuracy: 0.8520 - val_loss: 0.3168\n",
      "Epoch 7/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8727 - loss: 0.3075 - val_accuracy: 0.8580 - val_loss: 0.3164\n",
      "Epoch 8/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step - accuracy: 0.8734 - loss: 0.3010 - val_accuracy: 0.8592 - val_loss: 0.3152\n",
      "Epoch 9/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8760 - loss: 0.2944 - val_accuracy: 0.8592 - val_loss: 0.3161\n",
      "Epoch 10/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 30ms/step - accuracy: 0.8798 - loss: 0.2928 - val_accuracy: 0.8604 - val_loss: 0.3128\n"
     ]
    }
   ],
   "source": [
    "# # Compiles the mode and trains it\n",
    "\n",
    "# model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# history = model.fit(train_set, \n",
    "#                     validation_data=val_set, \n",
    "#                     epochs=10, \n",
    "#                     callbacks=[\n",
    "#                         tf.keras.callbacks.ModelCheckpoint(\n",
    "#                             \"./model_weights/my_universal_sentence_encoder_model.weights.h5\", \n",
    "#                             monitor='val_accuracy', \n",
    "#                             save_best_only=True, \n",
    "#                             save_weights_only=True)\n",
    "#                         # tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', restore_best_weights=True)]\n",
    "#                     ])\n",
    "\n",
    "# # Loads back the weights of the best model\n",
    "# model.load_weights(\"./model_weights/my_universal_sentence_encoder_model.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaddeb9-d4c1-495f-913c-4b1261bcb5f3",
   "metadata": {},
   "source": [
    "The validation accuracy of the above model has reached to 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cd32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluates model over test set\n",
    "# model_test_performance = model.evaluate(test_set)\n",
    "\n",
    "# print(\"Test Performance [Accuracy]: {:.1f}%\".format(model_test_performance[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7c589-afe9-4cbc-840d-beecba02f8cf",
   "metadata": {
    "id": "7fc7c589-afe9-4cbc-840d-beecba02f8cf"
   },
   "source": [
    "## Observations\n",
    "\n",
    "- How was the labels inferred subdirectories without reading from other directories of non-interest?\n",
    "\n",
    "- What were the major considerations to load data using TensorFlow dataset?\n",
    "\n",
    "- Why was a tokenizer used? How did it preprocess the input string? How length of all input string were made same?\n",
    "\n",
    "- Why did the model performed poorly when masking was not enabled? Change change was made into model to improve prediction performance?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
