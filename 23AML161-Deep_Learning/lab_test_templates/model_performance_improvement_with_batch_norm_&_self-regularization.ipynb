{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d75a41c",
   "metadata": {},
   "source": [
    "# MODEL PERFORMANCE IMPROVEMENT WITH BATCH NORMALIZATION & SELF-REGULARIZATION\n",
    "\n",
    "**_Experimenting with techniques that solves deep neural network (DNN) training related problems such as gradient vanishing/exploding and slow training._**\n",
    "\n",
    "**List of Experiments:**\n",
    "\n",
    "1. Build a vanila DNN with layers using **_He_ initialization** and the **_Swish_ activation** function. Add 20 hidden layers each with 100 neurons and an output layer with 10 neurons with **_softmax activation_**. Train the network using **_Nadam_ optimization** and **early stopping** on the CIFAR10 dataset.\n",
    "\n",
    "2. Add **batch normalization** and compare the learning curves to check if it **converges faster** than before. Also, check if model **prediction performance improved** and if batch normalization **affected training speed**.\n",
    "\n",
    "3. **[OPTIONAL]** Experiment if layers with **_SELU activation_** can **self-regulate** a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d070e",
   "metadata": {},
   "source": [
    "## Imports Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15dc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b204eb3",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d69f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape: tuple,\n",
    "    hidden_layers: list,\n",
    "    output_layer,\n",
    "    ):\n",
    "\n",
    "    # Resets all the keras states\n",
    "    tf.random.set_seed(42)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    for hidden_layer in hidden_layers:\n",
    "        model.add(hidden_layer)\n",
    "\n",
    "    model.add(output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x, y, optimizer, loss, metrics, batch_size=32, \n",
    "                epochs=1, callbacks=None, validation_data=None):\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    history = model.fit(\n",
    "        x, y, batch_size=batch_size, epochs=epochs, callbacks=callbacks, validation_data=validation_data)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df067c",
   "metadata": {},
   "source": [
    "## Data Ingestion & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd85a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the CIFAR10 dataset that contains 50,000 32x32-pixels color training images and 10,000 test images, \n",
    "# labeled over 10 categories. See more info at the CIFAR homepage https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pixel value range in train set: [{X_train_full.min()} - {X_train_full.max()}]\")\n",
    "print(f\"Pixel value range in test set: [{X_test.min()} - {X_test.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887b8aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts out validation set from the train set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=5000, random_state=42, stratify=y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd81998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c728528",
   "metadata": {},
   "source": [
    "Try executing the following command from terminal to watch live learning curve across all exprements.\n",
    "\n",
    "`tensorboard --logdir=<logs_path> --port=6006 --bind_all`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965e7a4",
   "metadata": {},
   "source": [
    "## Experiment #1\n",
    "**_Using better layer initialization, activation and optmization with early stopping_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d850b8",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of 20 hidden layers each with 100 units, \"swish\" activation and \"he_normal\" as kernel initializer\n",
    "\n",
    "hidden_layers = []\n",
    "for _ in range(20):\n",
    "    hidden_layers.append(#CODE HERE\n",
    "                         )\n",
    "\n",
    "# Also creates a dense layer as output with 10 units with \"softmax\" activation\n",
    "output_layer = #CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the model specifying the shape of the input as a tuple, hidden layers and output layer\n",
    "\n",
    "model1 = build_model(\n",
    "    input_shape=#CODE HERE,\n",
    "    hidden_layers=#CODE HERE,\n",
    "    output_layer=#CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a551753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Prints the model summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76c5ef",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures a list of callbacks \n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"./models/cifar-10/checkpoints/cifar-10.weights.keras\", save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(f\"./models/cifar-10/logs/run-{time.strftime(\"%Y.%m.%d_%H.%M.%S\")}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model1 making a call to `train_model` passing model, train set with labels,\n",
    "# `Nadam`optimzer with 5e-4 as learning_rate, \"sparse_categorical_crossentropy\" as loss,\n",
    "# [\"accuracy\"] as metrics, 100 as epochs, callbacks and validation set with labels\n",
    "history1 = train_model(#CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ba07e",
   "metadata": {},
   "source": [
    "### Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the learning curves\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15, 4))\n",
    "\n",
    "ax1.plot(history1.history[\"loss\"], \"b-\", label=\"Train loss\")\n",
    "ax1.plot(history1.history[\"val_loss\"], \"r-\", label=\"Validation loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Train vs. Validation loss\")\n",
    "\n",
    "ax2.plot(history1.history[\"accuracy\"], \"b-\", label=\"Train accuracy\")\n",
    "ax2.plot(history1.history[\"val_accuracy\"], \"r-\", label=\"Validation accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"Train vs. Validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lowest validation loss at epoch: {np.argmin(history1.history[\"val_loss\"]) + 1}\")\n",
    "\n",
    "print(f\"Highest validation accuracy of {max(history1.history[\"val_accuracy\"])*100:.2f}% \\\n",
    "      reached at epoch {np.argmax(history1.history[\"val_accuracy\"]) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests prediction performance on test set\n",
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ccaca",
   "metadata": {},
   "source": [
    "Record the above results to compare the same from other next experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75a039",
   "metadata": {},
   "source": [
    "## Experiment #2\n",
    "**_Adding batch normalization for faster convergence and better model performance_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca1367",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b77c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list of 20 pairs of hidden layers each with a batch normalization layer and dense\n",
    "# layer with 100 units, \"swish\" activation and \"he_normal\" as kernel initializer\n",
    "\n",
    "hidden_layers = []\n",
    "for _ in range(20):\n",
    "    hidden_layers.append(#CODE HERE)\n",
    "    hidden_layers.append(#CODE HERE)\n",
    "\n",
    "# Adds batch normalization just before the last dense output layer\n",
    "hidden_layers.append(#CODE HERE)\n",
    "\n",
    "# Creates a dense layer as output with 10 units with \"softmax\" activation\n",
    "output_layer = #CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c15b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the model specifying the shape of the input as a tuple, hidden layers and output layer\n",
    "model2 = build_model(\n",
    "    input_shape=#CODE HERE,\n",
    "    hidden_layers=#CODE HERE,\n",
    "    output_layer=#CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Prints the model summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1e457",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"./models/cifar-10/checkpoints/cifar-10.weights.keras\", save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(f\"./models/cifar-10/logs/run-{time.strftime(\"%Y.%m.%d_%H.%M.%S\")}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0f6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model2 making a call to `train_model` passing model2, train set with labels,\n",
    "# `Nadam`optimizer with 5e-4 as learning_rate, \"sparse_categorical_crossentropy\" as loss,\n",
    "# [\"accuracy\"] as metrics, 100 as epochs, callbacks and validation set with labels\n",
    "history2 = train_model(#CODE HERE\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273e2f5",
   "metadata": {},
   "source": [
    "### Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the learning curves\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, figsize=(15, 4))\n",
    "\n",
    "ax1.plot(history1.history[\"loss\"], \"c--\", label=\"Train loss (without BN)\")\n",
    "ax1.plot(history1.history[\"val_loss\"], \"y--\", label=\"Validation loss (without BN)\")\n",
    "\n",
    "ax1.plot(history2.history[\"loss\"], \"b-\", label=\"Train loss (with BN)\")\n",
    "ax1.plot(history2.history[\"val_loss\"], \"r-\", label=\"Validation loss (with BN)\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Train vs. Validation loss\")\n",
    "\n",
    "\n",
    "ax2.plot(history1.history[\"accuracy\"], \"c--\", label=\"Train accuracy (without BN)\")\n",
    "ax2.plot(history1.history[\"val_accuracy\"], \"y--\", label=\"Validation accuracy (without BN)\")\n",
    "\n",
    "ax2.plot(history2.history[\"accuracy\"], \"b-\", label=\"Train accuracy (with BN)\")\n",
    "ax2.plot(history2.history[\"val_accuracy\"], \"r-\", label=\"Validation accuracy (with BN)\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"Train vs. Validation accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lowest validation loss at epoch: {np.argmin(history2.history[\"val_loss\"]) + 1}\")\n",
    "\n",
    "print(f\"Highest validation accuracy of {max(history2.history[\"val_accuracy\"])*100:.2f}% \\\n",
    "      reached at epoch {np.argmax(history2.history[\"val_accuracy\"]) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1654ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests prediction performance on test set\n",
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e62ed1",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Write observations on the faster convergence, if so, of the model with BN layers.\n",
    "\n",
    "- Did the model with BN layers come up as a better model?\n",
    "\n",
    "- Compare the average time both the model took to complete each epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
