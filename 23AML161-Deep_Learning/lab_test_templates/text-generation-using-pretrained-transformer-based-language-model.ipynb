{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974",
   "metadata": {
    "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974"
   },
   "source": [
    "# TEXT GENERATION USING PRETRAINED TRANSFORMER-BASED LANGUAGE MODEL\n",
    "\n",
    "_**Using a pretrained transformer-based language model with Hugging Face Transformer library to generate a new story-like text against a prompt like \"Once upon a time...\".**_\n",
    "\n",
    "This experiment uses a GPT2 - a small generative pretrained transformer based small language model from OpenAI (https://huggingface.co/openai-community/gpt2). It has just 124 million parameters - the lowest number of parameters in GPT-2 family of models and makes inferences tractable over commodity computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7dfec9",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prompt text for model to follow to continue generating the content\n",
    "prompt = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de67281",
   "metadata": {},
   "source": [
    "## Text Generation using Pipeline (Inference API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets seed in random, numpy, tf and/or torch (if installed)\n",
    "set_seed(42)\n",
    "\n",
    "# Initialize an instance of pipeline by calling `pipeline` method and passing \n",
    "# argument \"text-generation\" in `task` parameter and \"gpt2\" in `model` parameter\n",
    "\n",
    "generator_pipeline = # Write code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd740b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate content (sequences) based on the prompt provided by calling\n",
    "# method `generator_pipeline` and passing the `prompt` to parameter `text_inputs`,\n",
    "# 200 in `max_length` to specify maximum number of tokens in the generated text, \n",
    "# `True` to `truncation` to enable truncation beyond mentioned length, \n",
    "# and 5 to `num_return_sequences` as total number of generated sequences to return\n",
    "\n",
    "# NOTE: The following steps may take few minutes to complete\n",
    "\n",
    "generated_sequences = # Write code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacdb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the pipeline generated sequences\n",
    "\n",
    "for idx, sequence in enumerate(generated_sequences):\n",
    "    print(idx+1, \"\\t\" + sequence['generated_text'])\n",
    "    print('-' * 80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a237b",
   "metadata": {},
   "source": [
    "## Text Generation using Model-Specific Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb18d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT2 model transformer with a language modeling head \n",
    "# (linear layer with weights tied to the input embeddings) on top by calling\n",
    "# method `from_pretrained` of class `TFGPT2LMHeadModel` and passing the arguments\n",
    "# \"gpt2\" to first parameter as name of pretrained model and `False` to parameter\n",
    "# `use_safetensors` as Keras 2.x compatibility in Keras 3.x installation\n",
    "\n",
    "model = # Write code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model-specific tokenizer by calling method `from_pretrained` of\n",
    "# class `GPT2Tokenizer` and passing argument \"gpt2\" as its parameter\n",
    "\n",
    "tokenizer = # Write code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encodes the prompt text by calling `encode` method of tokenizer\n",
    "# instance created in the previous step and passing the prompt as first parameter for encoding,\n",
    "# `False` to `add_special_tokens` for not to add special tokens such as 'SOS' and 'EOS' automatically,\n",
    "# and \"tf\" to `return_tensors` to return TensorFlow `tf.constant` objects\n",
    "\n",
    "encoded_prompt = # Write code\n",
    "\n",
    "# Prints the encoded prompt\n",
    "print(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de162cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequences by calling method `generate` on the model instance created\n",
    "# earlier and passing the encoded prompt to parameter `input_ids` as prompt for\n",
    "# the generated sequence to follow, `True` to `do_sample` to enable sampling the\n",
    "# next token (from the distribution over the vocabulary instead of choosing the \n",
    "# most likely next token), 200 to `max_length` to restrict the length of the \n",
    "# generated sequence, and 5 to `num_return_sequences` as total number of sequences \n",
    "# to be generated\n",
    "\n",
    "generated_sequences = # Write code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [OPTIONAL] Prints the encodings of one of the generated sequences [for reference]\n",
    "generated_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2466026-4f57-4e4a-9e6c-5025ac352046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decodes all the generated sequences\n",
    "for idx, sequence in enumerate(generated_sequences):\n",
    "    text = # Write code to call `decode` method of the tokenizer instance passing \n",
    "            # each sequence as first parameter, and `True` to parameter\n",
    "            # `clean_up_tokenization_spaces` to removes space artifacts inserted while \n",
    "            # encoding the sequence, e.g, \"state-of-the-art\" gets encoded as \"state - of - the - art\".\n",
    "\n",
    "    print(idx+1, \"\\t\" + text)\n",
    "    print(\"-\" * 80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c49f0",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- What type of pretrained model was considered for this text generation taks? What was its major characteristics?\n",
    "\n",
    "- What type of approaches were considered for inferencing in terms of abstracting complexity of text generation using the Transformer library?\n",
    "\n",
    "- Which approach was easiest? Why was it easy compared to the other approach?\n",
    "\n",
    "- Both the approaches supported not just one but a sequence of generated text. How did both the approaches support generating sequence of text instead of just one?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
