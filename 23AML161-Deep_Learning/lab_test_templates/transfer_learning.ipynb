{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f20911",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING\n",
    "\n",
    "**_Experimenting with Transfer Learning for a classification task._**\n",
    "\n",
    "In transfer learning, features learned on one problem are taken and leveraged them on a new and similar problem. It is usually done for tasks where dataset has too little to train a full-scale model from scratch. \n",
    "\n",
    "**The Experiment:**\n",
    "\n",
    "- Loads _MobileNetV2_ model - pretrained on _imagenet_ dataset, as a base model taking all layers except the top one that is used for classification specific to ImageNet task.\n",
    "\n",
    "- Freezes all the layers in the base model to avoid destroying already learned parameters during training related to new task to classify cats and dogs.\n",
    "\n",
    "- Adds few new, trainable layers such as pooling, dropout and dense layer on top of the frozen layers for them to learn based on the new dataset.\n",
    "\n",
    "- Prepares the new dataset in a form that is acceptable by the base model.\n",
    "\n",
    "- Trains the head (newly added layers) on the new data over few epochs to get them trained.\n",
    "\n",
    "- Unfreezes the all the layers in the base model and fine-tunes the entire model by re-training it on the new data with a very low learning rate to achieve meaningful improvements by incrementally adapting the pretrained features to the new data.\n",
    "\n",
    "- Evaluates the model performance on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24d6d5",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f8e69",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520006be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches the \"cats vs. dogs\" dataset using TFDS\n",
    "# Only 40% of the data is used to show the effectiveness of transfer learning\n",
    "train_set, val_set, test_set = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],  # 40% for training, 10% for validation and 10% for test\n",
    "    as_supervised=True,                                         # Includes labels\n",
    ")\n",
    "\n",
    "print(\"Sample Counts:\")\n",
    "print(f\"Training: {train_set.cardinality()}, Validation: {val_set.cardinality()}, Test: {test_set.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a069d",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots few of the samples from the train set\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(15, 8))\n",
    "axes = axes.flatten()                                       # Flattens 2D array of axes to 1D for easier iteration\n",
    "for idx, (image, label) in enumerate(train_set.take(8)):    # Iterates through sample images to plot them\n",
    "    axes[idx].imshow(image)                                 # Displays image in corresponding subplot\n",
    "    axes[idx].set_title(int(label))                         # Sets title of subplot to image label\n",
    "    axes[idx].axis(\"off\")                                   # Hides axis for a cleaner look\n",
    "\n",
    "plt.suptitle(\"Samples from Cats. vs. Dogs Training set\", fontsize=16)   # Sets figure title\n",
    "plt.tight_layout()                                          # Adjusts layout to prevent overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fb7af",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b00cb8",
   "metadata": {},
   "source": [
    "As the images size varies and each pixel consists of 3 integer values between 0 and 255 (RGB level values), all the images get resized to same size of 160x160 and pixel values get normalized between -1 and 1 according to the requirement of the pretrained model. Additionally, random data augmentation is also applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571f482",
   "metadata": {},
   "source": [
    "**Resizing images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "resizer = # Code here to use `tf.keras.layers.Resizing` as function to resize image to 160x160\n",
    "\n",
    "# Maps each sample from the each of the datasets and performs resizing\n",
    "# Perform the image resizing by calling `map` lambda function of each dataset \n",
    "# passing resizer (with labels)\n",
    "\n",
    "train_set = # Code here\n",
    "val_set = # Code here\n",
    "test_set = # Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6492c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirms the resizing just by taking one sample with an index\n",
    "next(iter(train_set))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed7334",
   "metadata": {},
   "source": [
    "**Augmenting Data**\n",
    "\n",
    "Applying data augmentation to generate samples artificially by applying random and realistic transformations to the training images, such as random horizontal flipping or small random rotations helping the model to get exposed to different aspects of the training data while slowing down overfitting. It makes sense when dataset is not large or not balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the sequence of operations (once again taking layers as utulity functions)\n",
    "augmentation_layers = [\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "]\n",
    "\n",
    "def data_augmentation(x):\n",
    "    \"\"\"\n",
    "    Augments the image by flipping and rotating it\n",
    "    \"\"\"\n",
    "    for layer in augmentation_layers:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "# Use `data_augmentation` function passing it to train set's `map` lambda function to augment the images\n",
    "train_set = # Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865e92a",
   "metadata": {},
   "source": [
    "**Batching Data and Optimizing Loading Speed using Prefetching and Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756f885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = train_set.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n",
    "val_set = val_set.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n",
    "test_set = test_set.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfe26e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate `tf.keras.applications.MobileNetV2` as base model specifying \n",
    "# (160, 160, 3) as `input_shape`, 'False` to `include_top` to not to include top layer and\n",
    "# `imagenet` as `weights` to initialize the model with specific weights\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    # Code here\n",
    "    ...\n",
    "    ...\n",
    ")\n",
    "\n",
    "# Code to freeze all layers in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target model out of base model\n",
    "\n",
    "inputs = # Code to initialize the model input with expectation of `shape` (160, 160, 3)\n",
    "\n",
    "x = # Code to pass `inputs` to routine `tf.keras.applications.mobilenet_v2.preprocess_input` to scale input pixels between -1 and 1\n",
    "\n",
    "\n",
    "x = # Code to pass `x` to base model `base_model`. Also set `training` parameter to `False` to keep model in inference mode\n",
    "\n",
    "x = # Code to call `tf.keras.layers.GlobalAveragePooling2D()` passing `x` to convert base model's multi-dimensional output (`base_model.output_shape[1:]`) to vectors\n",
    "\n",
    "x = # Code to call `tf.keras.layers.Dropout` passing argument 0.2 to initialize and then pass `x` to initialized class to regularize the network with 20% dropout\n",
    "\n",
    "outputs = # Call to `tf.keras.layers.Dense` with initialize a dense layer with 1 unit and then pass `x` to the layer\n",
    "\n",
    "model = # Create target model calling `tf.keras.Model` passing both `inputs` and `outputs` as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(show_trainable=True)      # Shows the model summary with both trainable and non-trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb6318",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d384f2",
   "metadata": {},
   "source": [
    "### Training Head of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eec0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model by calling model's `compile` method passing `adam` as `optimizer`, `tf.keras.losses.BinaryCrossentropy(from_logits=True)` as `loss`,\n",
    "# [tf.keras.metrics.BinaryAccuracy()] as metrics\n",
    "# Code here\n",
    "\n",
    "# Call model's `fit` method to fit the model on train set with 2 epochs. \n",
    "# Also pass validation set to parameter `validation_data` to measures the learning during training\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the model performance post head training\n",
    "\n",
    "post_head_train_perf = model.evaluate(test_set)\n",
    "\n",
    "print(f\"Test set performance after head training:\\n \\\n",
    "      Loss: {post_head_train_perf[0]:.2f}, Accuracy: {post_head_train_perf[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eae191",
   "metadata": {},
   "source": [
    "### Fine-tuning the Model\n",
    "Unfreezing the base model and training the entire model end-to-end with a low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66123636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to set (all layers of) base model as not-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d007ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e90a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model by calling model's `compile` method passing `tf.keras.optimizers.Adam(1e-5)` as `optimizer`, \n",
    "# `tf.keras.losses.BinaryCrossentropy(from_logits=True)` as `loss`,\n",
    "# [tf.keras.metrics.BinaryAccuracy()] as metrics\n",
    "# Code here\n",
    "\n",
    "# Call model's `fit` method to fit the model on train set with 5 epochs. \n",
    "# Also pass validation set to parameter `validation_data` to measures the learning during training\n",
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab48c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the model performance post fine-tuning the full model\n",
    "\n",
    "post_full_model_train_perf = model.evaluate(test_set)\n",
    "\n",
    "print(f\"Test set performance after full model training:\\n \\\n",
    "      Loss: {post_full_model_train_perf[0]:.2f}, Accuracy: {post_full_model_train_perf[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e71f1",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- Did the model learn over less data? Explain in detail.\n",
    "\n",
    "- Why is the reason data augmentation was used for?\n",
    "\n",
    "- List general workflow of transfer learning followed in this experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
