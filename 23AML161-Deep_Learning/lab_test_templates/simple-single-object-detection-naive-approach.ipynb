{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3df201",
   "metadata": {},
   "source": [
    "# Simple Single Object Detection - A Naive Approach\n",
    "\n",
    "**_Experimenting with a single object detection using subset of Caltech-101 dataset and transfer learning._**\n",
    "\n",
    "In this experiment, images of airplanes with thier bounding boxes are extracted from Caltech-101 dataset and then trained on a MobileNetV2 pretrained model with customization to detect the location of airplanes in new images.\n",
    "\n",
    "**The Experiment:**\n",
    "\n",
    "- Downloads Caltech-101 dataset (caltech-101.zip) from https://data.caltech.edu/records/mzrjq-6wc02. This dataset has pictures of objects belonging to 101 categories each containing about 40 to 800 images. The size of each image is roughly 300 x 200 pixels. The annotations stored as MATLAB script file (.mat) contain outlines (bounding box) of each object in these pictures.\n",
    "\n",
    "- Extracts the dataset from the downloaded compressed file caltech-101.zip.\n",
    "\n",
    "- Sets the root paths for airplanes images and and annotations.\n",
    "\n",
    "- Loads the data (image path and respective bounding boxes) into an intermediate datastructure by reading bounding boxes from MATLAB .mat annotation file and rescaling the boxes.\n",
    "\n",
    "- Writes helper functions to show images and to draw bounding boxes.\n",
    "\n",
    "- Samples randomly from the metadata and check visually if the above two helper functions work.\n",
    "\n",
    "- Uses a lightweight relevant pretrained model e.g. MobileNetV2 excluding the top layers used for ImageNet classification task. Add task specific top layers.\n",
    "\n",
    "- Uses an data loader such as TensorFlow Dataset to serve data efficiently while model training.\n",
    "\n",
    "- Seperates validation set froom training set.\n",
    "\n",
    "- Compiles and fit the model with early stopping.\n",
    "\n",
    "- Plots the learning curve.\n",
    "\n",
    "- Performs predictions on validation data and visualize these by plotting both images and associated predicted bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11455752",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a91b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                           # For file system related tasks\n",
    "import urllib.request as request, zipfile, tarfile  # For downloading file from Internet and extractions   \n",
    "import random\n",
    "from scipy.io import loadmat                        # For reading MATLAB (.mat) files\n",
    "from PIL import Image                               # For reading image files\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle            # Drawing rectangle shape on on plots\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec75b1",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a69ce",
   "metadata": {},
   "source": [
    "**Downloading Data Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\"\n",
    "\n",
    "# Creates dataset directory, if does not exist. \n",
    "# Set the path according to you preference or based on the directory the data already exists.\n",
    "data_dir = \"./../data/caltech-101\"\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "data_file_name = \"caltech-101.zip\"\n",
    "data_file_path = os.path.join(data_dir, data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads data file only if it does not exist\n",
    "\n",
    "# UNCOMMENT THE FOLLOWING LINES AND RUN ONLY IF THE EXTRACTED DATASET IS NOT AVAILABLE. \n",
    "# CHECK WITH THE INSTRUCTOR FIRST.\n",
    "\n",
    "# if not os.path.isfile(data_file_path):\n",
    "#     print(f\"Data file {data_file_path} could not be found. Downloading...\", end=\"\")\n",
    "#     try:\n",
    "#         request.urlretrieve(url, data_file_path)\n",
    "#         print(\"successful.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while downloading: {e}\")\n",
    "# else:\n",
    "#     print(\"Image file already exists. Downloading was skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61480715",
   "metadata": {},
   "source": [
    "**Decompressing Data File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19364c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompresses images file\n",
    "\n",
    "# UNCOMMENT THE FOLLOWING LINES AND RUN ONLY IF THE EXTRACTED DATASET IS NOT AVAILABLE. \n",
    "# CHECK WITH THE INSTRUCTOR FIRST.\n",
    "\n",
    "# if os.path.isfile(data_file_path):\n",
    "#     print(f\"Decompressing {data_file_path}...\", end=\"\")\n",
    "#     try:\n",
    "#         with zipfile.ZipFile(data_file_path, \"r\") as zipped:\n",
    "#             zipped.extractall(data_dir)\n",
    "#         print(\"successful.\")\n",
    "\n",
    "#         # Decompressing images tar file\n",
    "#         images_tarfile_path = os.path.join(data_dir, \"caltech-101\", \"101_ObjectCategories.tar.gz\")\n",
    "#         print(f\"Decompressing {images_tarfile_path}...\", end=\"\")\n",
    "#         try:\n",
    "#             with tarfile.open(images_tarfile_path, \"r:gz\") as tar:\n",
    "#                 tar.extractall(os.path.join(data_dir, \"caltech-101\"), filter=\"data\")\n",
    "#             print(\"successful.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred while decompressing: {e}\")\n",
    "#         finally:\n",
    "#             tar.close()\n",
    "\n",
    "#         # Decompressing annotations tar file\n",
    "#         annotations_tarfile_path = os.path.join(data_dir, \"caltech-101\", \"Annotations.tar\")\n",
    "#         print(f\"Decompressing {annotations_tarfile_path}...\", end=\"\")\n",
    "#         try:\n",
    "#             with tarfile.open(annotations_tarfile_path, \"r\") as tar:\n",
    "#                 tar.extractall(os.path.join(data_dir, \"caltech-101\"), filter=\"data\")\n",
    "#             print(\"successful.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred while decompressing: {e}\")\n",
    "#         finally:\n",
    "#             tar.close()\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while decompressing: {e}\")\n",
    "# else:\n",
    "#     print(f\"File {data_file_path} does not exist. Decompression was skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e9bc5",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250783d",
   "metadata": {},
   "source": [
    "Rename the following annotations folders as there are mismatches between images and annotations folder names.\n",
    "\n",
    "- Airplanes_Side_2 to airplanes\n",
    "- Faces_2 to Faces\n",
    "- Faces_3 to Faces_easy\n",
    "- Motorbikes_16 to Motorbikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the root paths for airplanes images and and annotations.\n",
    "images_dir = os.path.join(data_dir, \"caltech-101\", \"101_ObjectCategories\", \"airplanes\")\n",
    "annotations_dir = os.path.join(data_dir, \"caltech-101\", \"Annotations\", \"airplanes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f706f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_box(box, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Scales the bounding box on a unit square and converts box from\n",
    "    the format [y1, y2, x1, x2] to [x1, y1, width, height]\n",
    "    \"\"\"\n",
    "\n",
    "    box = [box[2], box[0], box[3]-box[2], box[1]-box[0]]\n",
    "\n",
    "    # Write code to scale down the variables in the bounding box in a unit scale based on the \n",
    "    # maximum length of either image width or height.\n",
    "    scale = # Write code\n",
    "    x, y, w, h = # Write code\n",
    "\n",
    "    ## Center aligns starting coordinates\n",
    "    x += (image_height - image_width) * scale / 2 if image_height > image_width else 0\n",
    "    y += (image_width - image_height) * scale / 2 if image_width > image_height else 0\n",
    "    \n",
    "    return [x, y, w, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f6fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data (image path and respective bounding boxes) into an intermediate datastructure \n",
    "# by reading bounding boxes from MATLAB .mat annotation file and rescaling them.\n",
    "\n",
    "metadata = #  Write code to initialize a dictionary as an intermediate data structure to hold meta information about the images and respective bounding boxes\n",
    "\n",
    "id = # Write code to initialize a simple counter to act as key to the metadata dictionary\n",
    "\n",
    "for file in os.listdir(images_dir):             # Iterates over the files in the airplane image folder\n",
    "   image_path = # Write code to get a path to the file for image loading\n",
    "   base_name = os.path.splitext(file)[0]        # Gets base name of image file [e.g. ../airplanes/image_0616.jpg to image_0616] to prepare path for associated .mat file\n",
    "   annotation_file = os.path.join(data_dir, \"caltech-101\", \"Annotations\", \"airplanes\", f\"annotation_{base_name[-4:]}.mat\")    # Gets path to annoation (.mat) file\n",
    "   if os.path.exists(annotation_file):          # Skips if the associated annotation file does not exist\n",
    "      metadata[id] = id                         # Sets the key against this meta information for later retrieval\n",
    "      mat_contents = # Write code to reads the content from the .mat file by passing it to function `loadmat`\n",
    "      with Image.open(image_path) as image:     # Loads the image to get image width and height to scale the bounding box accordingly\n",
    "         scaled_box = scale_box(mat_contents['box_coord'][0].tolist(), image.width, image.height)  # Scales the bounding box in a unit square\n",
    "      metadata[id] = # Write code to store scaled box and image path as dictionary values \"box\" and \"image_path\" against key identified by `id` in metadata dictionary\n",
    "      # Write code to increment the counter\n",
    "   else:\n",
    "         print(f\"Not found: {annotation_file}\") # Prints of an associated annotation file against an image file does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write code to get a random from the metadata to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(ax, image):\n",
    "    \"\"\"\n",
    "    Draws the image on a unit cube with (0, 0) at the top left\n",
    "    \"\"\"\n",
    "    ax.set(xlim=(0, 1), ylim=(1, 0), xticks=[], yticks=[], aspect=\"equal\")\n",
    "    image = # Write code to read the image from its path `image` by passing it to pyplot's method `imread`\n",
    "    height, width = # Write code to gets the image's height and width by reading first two dimention (considering the 3rd dimension is used for channels)\n",
    "    \n",
    "    # Pads the image so it fits inside the unit cube\n",
    "    hpad = (1 - height / width) / 2 if width > height else 0\n",
    "    wpad = (1 - width / height) / 2 if height > width else 0\n",
    "    extent = [wpad, 1 - wpad, 1 - hpad, hpad]\n",
    "    \n",
    "    # Write code to show the image by passing it as a first argument in the method `imshow` of the given figure's axis `ax` amd\n",
    "    # extent variable in named parameter `extent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(ax, box, color):\n",
    "    \"\"\"\n",
    "    Draws bounding box of a specific linewidth (lw), (edge) color (ec)\n",
    "    \"\"\"\n",
    "    x, y, w, h = box\n",
    "    ax.add_patch(Rectangle((x, y), w, h, lw=2, ec=color, fc=\"none\"))    # Draws the bounding box as rectangle with no filling color (fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction(image, predicted_box):\n",
    "    \"\"\"\n",
    "    Draws the both image and predicted bounding box on a unit cube with (0, 0) at the top left\n",
    "    utilising helper functions `draw_image()` and `draw_box()`.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=150)\n",
    "\n",
    "    # Write code to first draws the image containg object for which bounding box is to be predicted by calling method\n",
    "    # `draw_image` passsing into it the axis `ax` and the image as argument\n",
    "\n",
    "    # Write code to draw the predicted box by calling method `draw_box` and passing into it the \n",
    "    # axis `ax`, the predicted box and the color \"r\" as argument to draw the box in red\n",
    "\n",
    "    # Write code to show the plot by calling method `show` on instance of pyplot    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample randomly from the metadata and check visually if the above two helper functions work\n",
    "\n",
    "random_sample_id = random.randint(0, len(metadata)-1)\n",
    "sample = metadata[random_sample_id]\n",
    "\n",
    "ig, ax = plt.subplots(dpi=150)\n",
    "\n",
    "# Write code to extract image path and bounding box from the sample variable (dictionary) show both\n",
    "# image and plot the bounding box by calling method `draw_image` and `draw_box`, respectively.\n",
    "# Use 'b' as indicator to plot the bounding box in blue.\n",
    "# ...\n",
    "# ...\n",
    "# ...\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally shuffles the stored information before preparing data set for modeling\n",
    "random.shuffle(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e785c",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b662883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses a particular images size for which the target pretrained model (refer below) offers optimized model weights\n",
    "image_size = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a009d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 04:20:59.203902: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Instantiates the MobileNetV2 architecture and returns a an image classification model loaded with weights pre-trained on ImageNet.\n",
    "# Refer more details at https://keras.io/api/applications/mobilenet/#mobilenetv2-function\n",
    "\n",
    "# Write code to call method tf.keras.applications.MobileNetV2 by passing \n",
    "# expected input shape (3D) against parameter `input_shape`,\n",
    "# `False` against parameter `inc]lude_top` to exclude imagenet specific top layer, and\n",
    "# argument \"imagenet\" against parameter `weights` to load optimized weights of model pretrained on \"imagenet\" dataset\n",
    "\n",
    "base_model = # ...\n",
    "    # ...\n",
    "    # ...\n",
    "    # ...\n",
    "\n",
    "# Write code to set (parameters of the) base model non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74499203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model out of base model\n",
    "\n",
    "inputs = # Write code to create model's input using tf.keras.Input passing correct 3D `shape`\n",
    "x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)     # Scales input pixels between -1 and 1 before passing them to the model.\n",
    "x = # Write code to pass the `inputs` into tf.keras.applications.mobilenet_v2.preprocess_input to scale input pixels between -1 and 1 before passing them to the model.\n",
    "x = # Write code to pass processed input to base_model\n",
    "\n",
    "# Makes basemodel outputs smaller and then flattens the output features\n",
    "x = # Write code to create a tf.keras.layers.Conv2D layer with 512 filters, (3, 3) kernel size, (2, 2) strides, and pass base model's output into it to convolve its spatial dimension\n",
    "x = # Write code to flatten the convolutional layer's 3-D output (feature maps) into 1-D using tf.keras.layers.Flatten and passing output of the Conv2D layers output\n",
    "\n",
    "# Passes our flattened data through three densely connected layers\n",
    "x = # Write code to create a dense layer with 128 units and \"relu\" activation using tf.keras.layers.Dense method and passing flattended output\n",
    "x = # Write code to create a dense layer with 64 units and \"relu\" activation using tf.keras.layers.Dense method and output from the previous dense layer\n",
    "x = # Write code to create a dense layer with 32 units and \"relu\" activation using tf.keras.layers.Dense method and output from the previous dense layer\n",
    "\n",
    "head = # Write code to create a dense layer with 4 units [for start coordinates (x, y), width and height of the predicted\n",
    "# bounding box] each with sigmoid activation to ensure outputs range between 0 and 1 to scale later.\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=head)                 # Creates target model combining inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks the model summary before proceeding for model training\n",
    "# Ensure non-trainable weights shown in red for locked base model.\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient data loader to serve data efficiently while model training\n",
    "\n",
    "resizer = tf.keras.layers.Resizing(image_size, image_size)      # A function to make the size of all input images same\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    \"\"\"\n",
    "    Loads and resizes an input image from its path\n",
    "    \"\"\"\n",
    "    x = tf.io.read_file(path)\n",
    "    x = tf.image.decode_jpeg(x, channels=3)\n",
    "    return resizer(x)\n",
    "\n",
    "# First loads path for all images into TensorFlow Dataset and then\n",
    "# loads the images against each of the image path\n",
    "images = tf.data.Dataset.from_tensor_slices([v[\"image_path\"] for v in metadata.values()])\n",
    "images = images.map(load_image, num_parallel_calls=8)\n",
    "\n",
    "# Similarly, loads the bounding box for all images\n",
    "labels = tf.data.Dataset.from_tensor_slices([v[\"box\"] for v in metadata.values()])\n",
    "\n",
    "# Combines these two dataset into one\n",
    "dataset = tf.data.Dataset.zip(images, labels)\n",
    "\n",
    "# Seperates out 20% of samples as validation set, and then\n",
    "# sets batch size and enables prefetching for each of the datasets\n",
    "val_set = dataset.take(160).batch(32).prefetch(2)\n",
    "train_set = dataset.skip(160).batch(32).prefetch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e09531",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a18fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to compile the model by calling model.compile method passing tf.keras.optimizers.Adam \n",
    "# as `optimizer` (with learning rate 1e-4) and \"mse\" as `loss` function\n",
    "# ...\n",
    "# ...\n",
    "\n",
    "# Write code to fit the model by calling model.fit and passing train set, validation set to `validation_data`,\n",
    "# 50 to `epochs` and `callbacks` with a list containing an early stopping object initialized with\n",
    "# 10 as `patience`, \"val_loss\" as `monitor`, \"min\" as `mode` and \"True\" as `restore_best_weights` arguments\n",
    "history = model.fit(# ...\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the learning curve\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"Training loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs predictions on the validation data\n",
    "val_predictions = model.predict(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75abf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks one of the predictions to ensure \n",
    "val_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizes the predicted bounding boxes of any random image from validation set\n",
    "# [Note: Run this cell multiple time to check predictions against different samples]\n",
    "\n",
    "random_sample_id = random.randint(0, 160-1)\n",
    "\n",
    "predicted_box = val_predictions[random_sample_id]\n",
    "\n",
    "image_path = metadata[random_sample_id][\"image_path\"]   # Gets the image file path from metadata as it was not required to be stored in TensorFlow Dataset for modeling\n",
    "\n",
    "print(f\"Image path: {image_path}\")\n",
    "draw_prediction(image_path, predicted_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7c7a5",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- How was the subset prepared for both images and its annotations to make it ready for model training?\n",
    "\n",
    "- Why was an intermediate data structure prepared to store dataset, and how was the structure like and what did it contain?\n",
    "\n",
    "- Which helper functions were created and why were they created for?\n",
    "\n",
    "- Why was a pretrained model used as a base model? How was a custom model built on top of it?\n",
    "\n",
    "- Which data loader was used to load the data to fed into model for training? Why was an data loader used instead of manually fed the data into model for training? Why was early stopping used and how it was configured?\n",
    "\n",
    "- Explain the learning curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
