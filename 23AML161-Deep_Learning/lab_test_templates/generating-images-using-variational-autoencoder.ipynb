{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "de27da6e",
      "metadata": {
        "id": "de27da6e"
      },
      "source": [
        "# GENERATING IMAGES USING VARIATIONAL AUTOENCODER"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e0d62f",
      "metadata": {},
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eeaf462",
      "metadata": {
        "id": "4eeaf462"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ji3IuLa80AQk",
      "metadata": {
        "id": "ji3IuLa80AQk"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"     # For Keras 2.x compatibility in Keras 3.x installation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e47945e",
      "metadata": {
        "id": "7e47945e"
      },
      "source": [
        "## Data Acquisition & Preparation\n",
        "\n",
        "Uses Fashion MNIST data set containing 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. Pixel value of the images ranges from 0 through 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f6cf02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89f6cf02",
        "outputId": "ea39bbae-a060-4d25-8f87-e6d8e25aeb35"
      },
      "outputs": [],
      "source": [
        "# Loads the dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TdJHznD905mw",
      "metadata": {
        "id": "TdJHznD905mw"
      },
      "outputs": [],
      "source": [
        "# Normalize the full train set and test set considering the maximum pixel value \n",
        "# mentioned in the comment cell above to range values for both the sets between 0 and 1\n",
        "\n",
        "X_train_full = # ...\n",
        "X_test = # ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2lfhqc8o07xc",
      "metadata": {
        "id": "2lfhqc8o07xc"
      },
      "outputs": [],
      "source": [
        "# Split the train set further to separate 10000 samples as validation set from full train set stratifically\n",
        "# by calling method `train_test_split` and passing full train set with labels seperated by comma in\n",
        "# the first two parameters, then pass 10000 in `test_size` and label `y_train_full` in `stratify` parameter.\n",
        "# Additionally, an integer can also be passed in parameter.\n",
        "\n",
        "X_train, X_val, y_train, y_val = # ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e507eff6",
      "metadata": {
        "id": "e507eff6"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967a9a7d",
      "metadata": {
        "id": "967a9a7d"
      },
      "outputs": [],
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom layer to sample the coding given mean vector (μ) and log variance vector (γ) \n",
        "    where γ = log(σ^2). σ is standard deviation vector.\n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        mean, log_var = inputs\n",
        "        return tf.random.normal(tf.shape(log_var)) * tf.exp(log_var / 2) + mean"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96629c7b",
      "metadata": {
        "id": "96629c7b"
      },
      "source": [
        "**Modeling Variational Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31369478",
      "metadata": {
        "id": "31369478"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "codings_size = 10   # Length of the mean vector and log variance vector representing latent space of the input data. \n",
        "\n",
        "# Use Keras functional APIs as instructed below to create a variational encoder\n",
        "inputs = # Create an input layer by initializing `tf.keras.layers.Input` with a \n",
        "        # `shape` of 2D input image. Shape should a list of sizes.\n",
        "\n",
        "Z = # Initialize a `tf.keras.layers.Flatten` layer and pass the `inputs` as an argument\n",
        "    # to pass the images through it to flatten them into 1D.\n",
        "\n",
        "Z = # Create a dense layer by calling method `tf.keras.layers.Dense` with 150 as first \n",
        "    # parameter as number of output units and \"relu\" to `activation` parameter and then\n",
        "    # pass previous layer's output `Z` through the layer\n",
        "\n",
        "\n",
        "Z = # Create a dense layer by calling method `tf.keras.layers.Dense` with 100 as first \n",
        "    # parameter as number of output units and \"relu\" to `activation` parameter and then\n",
        "    # pass previous layer's output `Z` through the layer\n",
        "\n",
        "\n",
        "codings_mean = # Create a dense layer by calling method `tf.keras.layers.Dense` with output \n",
        "                # unit equal to the coding size as parameter without any activation function \n",
        "                # and then pass previous layer's output `Z` through the layer to get mean \n",
        "                # vector (μ) out of this layer\n",
        "\n",
        "codings_log_var = # Create another dense layer exactly similar to the way coding mean layer was \n",
        "                    # created in the previous step, but to create it for log variance vector (γ)\n",
        "\n",
        "codings = # Initialize sampling layer passing a list containing `codings_mean` and `codings_log_var` \n",
        "            # as output from the respective layers for samplling layer to sample random coding \n",
        "            # vector from Gaussian distribution with with mean μ and standard deviation σ\n",
        "\n",
        "variational_encoder = # Finally, combine the inputs and outputs by calling method `tf.keras.Model` \n",
        "                        # and passing a list containing `inputs` layer to parameter `inputs` and \n",
        "                        # another list containing `codings_mean`, `codings_log_var` and `codings` \n",
        "                        # to parameter `outputs` to create a variational encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a0faee",
      "metadata": {
        "id": "74a0faee"
      },
      "source": [
        "**Modeling Variational Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdbc3f8f",
      "metadata": {
        "id": "fdbc3f8f"
      },
      "outputs": [],
      "source": [
        "decoder_inputs = # Create an input layer by calling `tf.keras.layers.Input` and passing \n",
        "                    # a list containing coding size\n",
        "\n",
        "x = # Create a dense layer calling method `tf.keras.layers.Dense` and passing 100 to first parameter as number of output units\n",
        "# and \"relu\" as activation function. Then pass `decoder_inputs` through it.\n",
        "\n",
        "x = # Create a dense layer calling method `tf.keras.layers.Dense` and passing 150 to first parameter as number of output units\n",
        "# and \"relu\" as activation function. Then pass `x` - the previous layer's output, through it.\n",
        "\n",
        "x = # Create a dense layer calling method `tf.keras.layers.Dense` and passing number of output units equal to the total number \n",
        "    # of pixels in 2D input image, and pass `x` - the output of the previous layer, to the initialized layer\n",
        "\n",
        "outputs = # Create a reshaping layer by calling method `tf.keras.layers.Reshape` passing a list containing shape of the 2D \n",
        "            # input image, then pass `x` - the output of the previous layer, to the initialized layer to reshape 1D input into 2D\n",
        "\n",
        "variational_decoder = # Finally, combine the inputs and outputs by calling method `tf.keras.Model` \n",
        "                    # and passing a list containing `decoder_inputs` layer to parameter `inputs` and \n",
        "                    # another list containing `outputs` to parameter `outputs` to create a variational decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132b49db",
      "metadata": {
        "id": "132b49db"
      },
      "source": [
        "**Combining Encoder & Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4bbd64",
      "metadata": {
        "id": "9b4bbd64"
      },
      "outputs": [],
      "source": [
        "_, _, codings = # Now, pass `inputs` to the encoder by calling variational encoder as a function and \n",
        "                # pass `inputs` as its parameter argument\n",
        "\n",
        "reconstructions = # Call variational decoder as a function and pass codings received in the previous \n",
        "                    # step as its parameter argument to get reconstructed images back\n",
        "\n",
        "variational_ae = # To combine variational encoder and variational decoder into a variational autoencoder,\n",
        "                # call `tf.keras.Model` and passing its `inputs` parameter with a list containing `inputs`, and\n",
        "                # its another parameter `outputs` with a list containing `reconstructions`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf268f4",
      "metadata": {
        "id": "caf268f4"
      },
      "outputs": [],
      "source": [
        "# Latent loss function to push the codings gradually migrate within the coding space\n",
        "# (also called the latent space) to end up looking like a cloud of Gaussian points.\n",
        "# It computes the latent loss for each instance in the batch, summing over the last axis.\n",
        "\n",
        "latent_loss = -0.5 * tf.reduce_sum(\n",
        "    1 + codings_log_var - tf.exp(codings_log_var) - tf.square(codings_mean),\n",
        "    axis=-1)\n",
        "\n",
        "variational_ae.add_loss(\n",
        "    tf.reduce_mean(latent_loss) / 784.  # Computes the mean loss over all the instances in the batch, \n",
        "                                        # followed by dividing the result by 784 to ensure it has the \n",
        "                                        # appropriate scale compared to the reconstruction loss.\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99982aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b99982aa",
        "outputId": "53077ca2-bd3f-48a0-dc9e-b89d33652ae7"
      },
      "outputs": [],
      "source": [
        "# Compiles model\n",
        "# Call `compile` method of the variational autoencoer mode passing \"mse\" as arguement to `loss` parameter\n",
        "# and \"nadam\" as argument to `optimizer` parameter to compile the model\n",
        "\n",
        "# Fits the model\n",
        "# Call `fit` method of the variational autoencoer mode passing the same train set as arguments to its first two parameters,\n",
        "# and 25 to `epochs`, 128 to `batch_size` and a 2-element tuple containing same validation set to `validation_data`.\n",
        "# to start model training.\n",
        "history = # ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aKHdp1o41WX2",
      "metadata": {
        "id": "aKHdp1o41WX2"
      },
      "source": [
        "## Generating Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9497kKV21XER",
      "metadata": {
        "id": "9497kKV21XER"
      },
      "outputs": [],
      "source": [
        "# Generates a few random codings and decodes them:\n",
        "\n",
        "ROWS = 3\n",
        "COLUMNS = 7\n",
        "\n",
        "# Samples random codings from a Gaussian distribution and decode them\n",
        "\n",
        "codings = tf.random.normal(shape=[ROWS * COLUMNS, codings_size])\n",
        "images = variational_decoder(codings).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e2de39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shows some of the fashion items sampled randomly from the generated images\n",
        "\n",
        "fig,ax = plt.subplots(ROWS, COLUMNS, figsize = (10,5))     # Figure to contain subplots in 5-rows and 9-columns arrangement\n",
        "ax = ax.ravel()                                 # Flattens the axes allowing accessing each axis contiguously\n",
        "for i in range(ROWS * COLUMNS):\n",
        "  ax[i].imshow(images[i], cmap = 'binary')      # Shows the image\n",
        "  ax[i].axis(\"off\")                             # Set the axis off for being non-relevant in this case\n",
        "\n",
        "fig.suptitle(\"Generated Fashion Items\")         # Sets title of the figure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1lqT7Xjp3JZj",
      "metadata": {
        "id": "1lqT7Xjp3JZj"
      },
      "source": [
        "## Observations\n",
        "\n",
        "- Why was Keras functional API used instead of sequential API?\n",
        "\n",
        "- Why was a custom layer instead of a regular or built-in layer to sample codings? What were its parameters for this layer to generate coding samples? \n",
        "\n",
        "- Which technique was followed to ensure coding migrate within the coding space? How did it work?\n",
        "\n",
        "- Explain in details all the steps followed to generate images and to visualize them."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
