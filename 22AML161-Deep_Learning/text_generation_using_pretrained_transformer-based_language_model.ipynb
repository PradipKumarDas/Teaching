{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974",
   "metadata": {
    "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974"
   },
   "source": [
    "# TEXT GENERATION USING PRETRAINED TRANSFORMER-BASED LANGUAGE MODEL\n",
    "\n",
    "_**Using a pretrained transformer-based language model with Hugging Face Transformer library to generate a new story-like text against a prompt like \"Once upon a time...\".**_\n",
    "\n",
    "This experiment uses a GPT2 - a small generative pretrained transformer based small language model from OpenAI (https://huggingface.co/openai-community/gpt2). It has just 124 million parameters - the lowest number of parameters in GPT-2 family of models and makes inferences tractable over commodity computers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7dfec9",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace6e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:40:05.250267: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-06 16:40:05.250705: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-06 16:40:05.290765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-06 16:40:06.264460: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-06 16:40:06.265171: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the prompt text for model to follow to continue generating the content\n",
    "prompt = \"Once upon a time in China\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de67281",
   "metadata": {},
   "source": [
    "## Text Generation using Pipeline (Inference API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Sets seed in random, numpy, tf and/or torch (if installed)\n",
    "set_seed(42)\n",
    "\n",
    "# Initializes an instance of pipeline - an abstraction of all other pipelines\n",
    "generator_pipeline = pipeline(\n",
    "    task=\"text-generation\",     # Task (such as \"summarization\", \"question-answering\") to return pipeline for\n",
    "    model=\"gpt2\"                # The model that will be used by the pipeline to make predictions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd740b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "# Generates content (sequences) based on the prompt provided\n",
    "# NOTE: The following steps may take few minutes to complete\n",
    "\n",
    "generated_sequences = generator_pipeline(\n",
    "    text_inputs = prompt,       # Input prompt for generated sequence to follow in semantic way\n",
    "    max_length=200,             # Controls the maximum number of tokens in the generated output including paddings for shorter outputs, if any\n",
    "    truncation=True,            # Enables truncation beyond mentioned length\n",
    "    num_return_sequences=5      # Total number of generated sequences to return\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cacdb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \tOnce upon a time in China, you can be sure that you will not be treated like a beggar.\n",
      "\n",
      "You can't just go around the country and buy a car and then go away. You must be accompanied by a person who knows what you can do.\n",
      "\n",
      "In general, you should stay in a hotel until you are in a decent place to live, for the time being. The longer you stay, the more likely you are to have problems. You may even go to bed in a hotel, when you are not in a good mood.\n",
      "\n",
      "If you are unhappy in the city, you need to get out of the country, and find a place to live.\n",
      "\n",
      "In general, you should stay in a hotel until you are in a decent place to live, for the time being. The longer you stay, the more likely you are to have problems. You may even go to bed in a hotel, when you are not in a good mood. If you are in poverty, you need to get out of the city, and find a place to live.\n",
      "\n",
      "A few things you should take into consideration when deciding to go to the city:\n",
      "\n",
      "If you are in a city where the government has decided to remove the government from the city, you have\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "2 \tOnce upon a time in China, China and India were both at war.\n",
      "\n",
      "It's a war that has lasted over 200 years, but it's not yet over. We have to be prepared for, if not at the moment, then right now.\n",
      "\n",
      "There's been a lot of talk of a Chinese military presence in the region and there are reports of a massive military presence in the Middle East, but this is a war with China.\n",
      "\n",
      "The United States, China, India, Japan, China have all become increasingly concerned that they're now at war.\n",
      "\n",
      "You are the largest military power in the world and you're responsible for a large part of the world's nuclear weapons.\n",
      "\n",
      "It's a war that's been raging for a very long time.\n",
      "\n",
      "The United States has also been involved in the war on terror, and there is a growing concern that India will be the target of a major attack, and the United States is prepared to go to war, as it has for many years, and I've said this repeatedly: We're not going to let that happen.\n",
      "\n",
      "I'm sure that the U.S. and the Chinese will be ready for any attack that comes their way.\n",
      "\n",
      "What are the chances of you going to get a nuclear\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "3 \tOnce upon a time in China, they were still doing this.\n",
      "\n",
      "\n",
      "The first time they came to the mainland, they would have to pay a huge amount because the government couldn't afford to spend the money. People in the countryside would be able to buy the goods in the market place, but then, as the prices started to rise, the government would want them to stay in the city to pay for what they had bought.\n",
      "\n",
      "\n",
      "The second time, they would have to be paid a huge amount because of the government's insistence that they leave the country for the mainland. They would get caught, they would be sent to the prison, and they might end up in the prison for over a year.\n",
      "\n",
      "\n",
      "The final time, they would have to pay a huge amount because of the Chinese government's insistence that they leave the country for the mainland. They would get caught, they would be sent to the prison, and they might end up in the prison for over a year.\n",
      "\n",
      "\n",
      "In this way, they would not have to go back to China for a while, and they would not have to go back to the mainland for any more.\n",
      "\n",
      "\n",
      "In this way, they would not have to go back to China for a while, and they would not have to go back to China\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "4 \tOnce upon a time in China, the government's attempt to establish a 'democratic' state in Mao Zedong's Great Leap Forward was met with a series of 'disappointing' developments.\n",
      "\n",
      "The 'democratic' state of the Chinese Communist Party was also beset by a series of problems. First of all, the Nationalists in power in Beijing were not really the people of the capital. At the time, the Communist Party of China was also the seat of the People's Liberation Army.\n",
      "\n",
      "Secondly, the People's Liberation Army was not quite a state, as it was not a democratic party. It was actually a dictatorship, as it was a dictatorship that was under the control of the People's Liberation Army.\n",
      "\n",
      "Thirdly, the People's Liberation Army was the'state'. The People's Liberation Army was a state because it was the seat of the People's Liberation Army. This was because the People's Liberation Army was the seat of the People's Liberation Army.\n",
      "\n",
      "It was the ruling class that established the People's Liberation Army. It was the ruling class that was able to control the people's affairs. They didn't have any other means to control the people's affairs than to have an army that could control any state.\n",
      "\n",
      "Therefore, it was a\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "5 \tOnce upon a time in China, the Emperor himself was a man of great merit, and in his youth was a skilled politician.\"\n",
      "\n",
      "The king was not a man of great merit, but a man of great talent.\n",
      "\n",
      "And so, in this city of the Sun City, the Emperor had a great deal of talent, which was a problem, for he had to do some things to improve the city's prosperity, and many things that were wrong in the city, he had to improve the city's fortunes.\n",
      "\n",
      "The emperor was very good at these things, the people were very good at these things, and he was very good at these things.\n",
      "\n",
      "The Emperor was very good at them, and the people were very good at them, and they were very good at them.\n",
      "\n",
      "\"But, are you sure that the Emperor does not know that he is the best thing in the world, when he is in charge of the emperor himself?\"\n",
      "\n",
      "\"He will certainly know that he is the best thing in the world, and no one else has the right to judge him.\"\n",
      "\n",
      "The Emperor seemed to understand, and as he spoke, the people nodded their heads to the Emperor and the emperor.\n",
      "\n",
      "The people were quite happy with the Emperor, and they were\n",
      "-------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the pipeline generated sequences\n",
    "\n",
    "for idx, sequence in enumerate(generated_sequences):\n",
    "    print(idx+1, \"\\t\" + sequence['generated_text'])\n",
    "    print('-' * 80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a237b",
   "metadata": {},
   "source": [
    "## Text Generation using Model-Specific Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb18d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initializes GPT2 model transformer with a language modeling head \n",
    "# (linear layer with weights tied to the input embeddings) on top\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\",   # Name of the pretrained model (or path to the saved model)\n",
    "    use_safetensors=False                   # Recommended to be set to `True`, but is set to `False` for Keras 2.x compatibility in Keras 3.x installation\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b8a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes model-specific tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ab46eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[7454 2402  257  640  287 2807]], shape=(1, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizes and encodes the prompt text\n",
    "encoded_prompt = tokenizer.encode(\n",
    "    prompt,                     # Prompt text to be encoded\n",
    "    add_special_tokens=False,   # Not to add special tokens such as 'SOS' and 'EOS' automatically\n",
    "    return_tensors=\"tf\"         # Returns TensorFlow tf.constant objects\n",
    "    )\n",
    "\n",
    "# Prints the encoded prompt\n",
    "print(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de162cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "# Generates sequences\n",
    "generated_sequences = model.generate(\n",
    "    input_ids = encoded_prompt,     # Prompt for the generated sequence to follow\n",
    "    do_sample=True,                 # Enables sampling the next token from the distribution over the \n",
    "                                    # vocabulary instead of choosing the most likely next token\n",
    "    max_length = 200,               # Restricts the length of the generated sequence\n",
    "    num_return_sequences = 5        # Total number of sequences to be generated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3499ae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
       "array([ 7454,  2402,   257,   640,   287,  2807,    25,   198,   198,\n",
       "           1,   818,  2805,    11,  1946,    11,   379,   257,  1171,\n",
       "       10542, 12007,   416,   262,  3999, 14884,  3615,    11, 10826,\n",
       "         286,   262, 14884,  3615,   286,  2807,    11,   511,  5694,\n",
       "        4606,    11,  1866,   286,   262,  7793,    65,  1434,   290,\n",
       "        3756,  1866,   286,   262,  4380,   338, 29235,  5407,   357,\n",
       "       45710,     8,  9141,   257,  2276,    12, 30280,  1181, 10474,\n",
       "         287,   262, 11618, 23200,   286,   406,  1530,   272,   284,\n",
       "       15402,   257,   366, 10057,  1512, 25070,     1,   329,   262,\n",
       "        7989,   290,  1099,   286,   705, 42017, 12148,     6,   355,\n",
       "        5625,   284,  1181,  2450,   287,  2807,   338,  2316,   351,\n",
       "        2869,    11,  4505,   290,  2520,  4969,  1399,   464,  2766,\n",
       "         286,   777,  2678,   788, 26443,   262,   685,    34,  4805,\n",
       "          60,   355,   705, 41131,   290, 29656,  4458,   366,  2215,\n",
       "         262, 45838,  1866,   286,   262,  4380,   338, 29235,  5407,\n",
       "         357, 45710,     8,  3414,   262,  2151,   338,  6323,   705,\n",
       "        6381,   363, 10237,     6,   351,   262,  1099,   286,   705,\n",
       "       42017, 12148,     6,   286,   262,  4380,   338,  2066,   286,\n",
       "        2807,    11,   262, 45838,  1866,   286,   262, 41770, 29235,\n",
       "        5407,   357, 45710,     8, 22798,   351,   705,  1040,   586,\n",
       "         286,   606,  4458,   366,  2215,   262,  1230,   286,  2869,\n",
       "        1444,   329,   281, 30258,   286,   663,  1992,    11,   262,\n",
       "        2766,   286], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [OPTIONAL] Prints the encodings of one of the generated sequences [for reference]\n",
    "generated_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2466026-4f57-4e4a-9e6c-5025ac352046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \tOnce upon a time in China:\n",
      "\n",
      "\"In March, 2014, at a public ceremony hosted by the Chinese Communist Party, representatives of the Communist Party of China, their Central Committee, members of the Politburo and leading members of the People's Liberation Army (PLA) attended a general-organized state assembly in the Beijing suburb of Lushan to condemn a \"systematic disregard\" for the principle and law of 'democracy promotion' as applied to state policy in China's relations with Japan, Australia and South Korea…The leaders of these countries then denounced the [CPR] as 'racist and divisive'. \"When the PLA members of the People's Liberation Army (PLA) announced the party's resolution 'disagreement' with the law of 'democracy promotion' of the People's Republic of China, the PLA members of the Peoples Liberation Army (PLA) reacted with 'insult of them'. \"When the government of Japan called for an impeachment of its President, the leaders of\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "2 \tOnce upon a time in China you need an intermediary who could help you. And those who can help are very, very wealthy. These are people who are able to invest heavily in China through investments in real estate, banking, and insurance companies, and who have an extremely good reputation there. They've shown great faith in the Chinese government's efforts and in many of the companies they've invested in.\n",
      "\n",
      "But they've also been very, very irresponsible in this area. Most of the deals they came into, and those that didn't, cost millions of dollars, and the people are going to be severely affected.\n",
      "\n",
      "That's why they don't have as much credibility as any other Chinese company in such areas. There are many companies with strong commercial ties in China. There's so many of them which have a financial stake in the country and they're very strong.\n",
      "\n",
      "AMY GOODMAN: It is the latest development in a saga that has been plagued by scandal on the Chinese mainland,\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "3 \tOnce upon a time in China you'll discover the power of the Chinese proverb, \"the Chinese proverb goes 'When you need help you tend to their problems.'\" The term is often used to illustrate the power of our minds. It's not a secret that we're not quite sure when to talk about our work and when to do something for us. We certainly do have the power to make changes. We can help, however, when needed – and we have power, too. We can become more powerful if we choose to do so.\n",
      "\n",
      "It's only natural to know what to do for ourselves. It's not an easy journey. One day, the world will decide we no longer need to look for help, that our work is worth more than our lives. A lot of this time is spent looking elsewhere for a way to pay attention to ourselves. It will be a huge waste of the time we have for this simple, small task.\n",
      "\n",
      "When you are ready to do something\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "4 \tOnce upon a time in China the Uighurs were given a form the word \"Jisrokeo\", a name for Mongolian.\n",
      "\n",
      "When first taken up to China by the king, as a joke or in the form of a token of approval, it was the best word of any type left at an all Chinese wedding to express how great the king looked.\n",
      "\n",
      "The Uighurs in China were used when their father died, but they were believed to have more in common with the Mongolian, the Sowanese and the Burmese, as well as the Chinese.\n",
      "\n",
      "People believe that there are several tribes in China on the Chinese side of the border, such as the Umi and Cokai, the Sowanese, the Gavong and Guo.\n",
      "\n",
      "Other names for the people are:\n",
      "\n",
      "Hai-Shia (Kai-Shia)\n",
      "\n",
      "Hail-Rong-Rong (K\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "5 \tOnce upon a time in China, the majority of the people have not heard of this, it is a fact.\"\n",
      "\n",
      "\"The fact that there was an earthquake in May and the fact that the police do not know how and when that happened in May is very troubling. If there was a fire, what did you do?\"\n",
      "\n",
      "In addition to his call to the top state police official, the other senior official met with a delegation of his personal representative, Deputy Director of Hong Kong's Central Police Office and Senior Police Inspector of Police. He claimed during the meeting that the chief of investigation had informed him that a group of four Hong Kong residents who had lived in an apartment on Aixin Road on Long Beach Road had attacked a group of people.\n",
      "\n",
      "The official also met on Long Beach Road for another five days with other senior officials to express his concern at how officials in the police chief's office behaved under his supervision and how it was possible for a group of young people, who\n",
      "-------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decodes all the generated sequences\n",
    "for idx, sequence in enumerate(generated_sequences):\n",
    "    text = tokenizer.decode(\n",
    "        sequence,                           # List of tokenized input ids.\n",
    "        clean_up_tokenization_spaces=True   # Removes space artifacts inserted while encoding the sequence, e.g, \"state-of-the-art\" gets encoded as \"state - of - the - art\".\n",
    "        )\n",
    "    print(idx+1, \"\\t\" + text)\n",
    "    print(\"-\" * 80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c49f0",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Smallest of GPT family of transformer based pretrained models with approximately 0.1 billion parameters were used for the above text generation task.\n",
    "\n",
    "- Both pipeline based and model-specific class based approach were considered to generate text.\n",
    "\n",
    "- Pipeline approach was as easy as specifying the task of interest and optionally the name of model to build the pipeline that encapsulates all low-level processeses such as tokenizing input (prompt), encoding the tokens, generating new text tokens and then decoding them back to get human-readable text.\n",
    "\n",
    "- Both the approaches supported not just one but a sequence of generated text."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
