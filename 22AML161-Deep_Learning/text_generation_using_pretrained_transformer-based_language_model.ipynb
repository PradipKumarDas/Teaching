{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974",
   "metadata": {
    "id": "0ff439c7-628e-4cf3-9fbc-a69be5815974"
   },
   "source": [
    "# TEXT GENERATION USING PRETRAINED TRANSFORMER-BASED LANGUAGE MODEL\n",
    "\n",
    "_**Use a pretrained transformer-based language model over Hugging Face Transformer library to generate a new story-like text against a prompt (input) like \"Once upon a time\".**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931eb04",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "The version of transformer-based pretrained small language model being used in this experiment is [Generative Pretrained Transformer (GPT) 2](https://huggingface.co/openai-community/gpt2) from OpenAI. This is the smallest [124 million parameters] version of GPT-2 family of models and it was selected to make inference tractable over commodity computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace6e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 21:57:31.995504: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-22 21:57:32.166708: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports required modules, classes and functions\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ae4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the prompt text for model to continue generating the content on\n",
    "prompt = \"Once upon a time in China\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de67281",
   "metadata": {},
   "source": [
    "## Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0245e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets seed in random, numpy, tf and/or torch (if installed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7e5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initializes an instance of pipeline which itself is an abstraction of all the other pipelines\n",
    "generator_pipeline = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd740b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generates content (sequences) based on the prompt provided\n",
    "# NOTE: The following steps may take few minutes to complete\n",
    "\n",
    "generated_sequences = generator_pipeline(\n",
    "    prompt,                             # Input prompt for generated sequence to follow in semantic way\n",
    "    max_length=200, truncation=True,    # Enables truncation beyond mentioned length\n",
    "    num_return_sequences=5              # Total number of generated sequences to return\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cacdb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \tOnce upon a time in China, where so many people still consider these as the Chinese Revolution's equivalent to civil war, it turns out that they were not the kind of guerrilla fighting that most observers would expect to see, at least not so many months later.\n",
      "\n",
      "What has been made clear, according to those who have been working for the Chinese government's most recent version of this strategy since 2002, is that the \"military revolution\" is not the Chinese way of fighting China. It is rather, according to the former government official, a strategy to bring what was left of the country's government under the control of a group of political leaders that included a group of people who were, in effect, \"protestors\" under a leadership of Mao Zedong. In other words, they wanted to get an alternative to the \"recession,\" of which they were aware, in part, because they were in favor of changing the nature of the Communist era, making a different sort of rule and\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "2 \tOnce upon a time in China it would have been impossible to live in an urban centre. It is easy to blame capitalism on lack of technology and the poor in China. These people need to be kept in a safe place for the future.\n",
      "\n",
      "People like Hanzhong Zhaowei who live on the outskirts of Beijing know the hardships they feel and the sacrifices they make in the future. They work and do everything with kindness that does not leave behind debts. They care for others and will never be bullied.\n",
      "\n",
      "The people of Hangzhou want to create the world's new \"great cities\" because there are more opportunities for them since they have a greater understanding of Chinese culture and are able to see the future. After all, they have lived for a long time in Hong Kong and Korea and want to lead an independent society now.\n",
      "\n",
      "In 2011, as a youth, Hanzhong was an outcast from his family but he did become the youngest student in his country when the government\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "3 \tOnce upon a time in China, there was a man who looked like an angel. He was very handsome indeed. He was very pale, and he had many eyes shining. His clothing was black. He was walking in a long black robe with a round collar and white neckline. That kind of clothing was not to be trusted in Hong Kong. The man had always maintained that the only way he could look like this was to make a move.\n",
      "\n",
      "Although he did not look in the mirror, he could still be seen on any night where he didn't see himself in disguise. When he was young, there was a place on the road called the Road of the Golden Age. Although there was a narrow road leading to it to some distance off, many people would never have thought about going through this narrow tunnel. It was so tight that they would jump in to see what it was like and what had happened. Many people wanted to go into this tunnel. People were able to follow the\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "4 \tOnce upon a time in China, I was asked to give speeches by Chinese officials and in one case to the Dalai Lama. There was no doubt that Xi would be a very important figure in this. When I returned to the United States, I had to deal with the press. I could not bring myself not to do it, but all I could do was see that there were more Americans there. And I felt very deeply regret at the time that there would have been fewer people here and at home.\n",
      "\n",
      "It was the same exact experience that I had about the \"Sons of David,\" in which there were two generations of Chinese, one that was descended from Chinese Jews, the other descended from Chinese nationalists and the other from the Chinese Communists. I began to write my book because I felt that this was a time of tremendous social change, where the Chinese people seemed not to be afraid about the Chinese. I felt the way I had been when I was 19. I wrote this book to\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "5 \tOnce upon a time in China, people in China who did not have a landlocked, but an exclusive landlocked economy could easily make all that money and then invest in all those things that they would soon go out of business… which is absolutely not what we are seeing here, but rather, is going to take away from China's growth potential if you are a country like China… and that is what people understand… [emphasis added] However, given the current circumstances the potential of these developments is very slim, and not very likely.\"\n",
      "\n",
      "A second point is being made about the possible consequences of such China-led sanctions on Russia: there is a much broader implication now than there was in 2011 about the potential to influence relations with the US. This was particularly apparent in the 2016 election in which Donald Trump managed to garner large, significant support in US-Russia relations under a new President Xi Jinping, but the United States is, at best, more likely to be the target or ally of\n",
      "-------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the pipeline generated sequences\n",
    "\n",
    "for idx, sequence in enumerate(generated_sequences):\n",
    "    print(idx+1, \"\\t\" + sequence['generated_text'])\n",
    "    print('-' * 80, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a237b",
   "metadata": {},
   "source": [
    "## Using Model-Specific Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb18d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Initializes GPT2 model transformer with a language modeling\n",
    "# head (linear layer with weights tied to the input embeddings) on top\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b8a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes model-specific tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ab46eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[7454 2402  257  640  287 2807]], shape=(1, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizes and encodes the prompt text\n",
    "encoded_prompt = tokenizer.encode(\n",
    "    prompt,                     # Prompt text to be encoded\n",
    "    add_special_tokens=False,   # Not to add special tokens such as 'SOS' and 'EOS' automatically\n",
    "    return_tensors=\"tf\"         # Returns TensorFlow tf.constant objects\n",
    "    )\n",
    "\n",
    "# Prints the encoded prompt\n",
    "print(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de162cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generates sequences\n",
    "generated_sequences = model.generate(\n",
    "    input_ids = encoded_prompt,     # Prompt for the generated sequence to follow\n",
    "    do_sample=True,                 # Enables sampling the next token from the distribution over the \n",
    "                                    # vocabulary instead of choosing the most likely next token\n",
    "    max_length = 200,               # Restricts the length of the generated sequence\n",
    "    num_return_sequences = 5        # Total number of sequences to be generated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3499ae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
       "array([ 7454,  2402,   257,   640,   287,  2807,   338,  2106,   262,\n",
       "         734,  5389,   561,   423,   550,   257,  1598,  3061,   286,\n",
       "       18591,   281,  4795, 18910,    11,  2158,   881,  2807,   550,\n",
       "         284, 33760,   284,   262,   584,  1735,    13,   383,  3999,\n",
       "         561,   635,   423,   550,   284, 33760,   262,  4925,   286,\n",
       "       18910,   550,   340,   587,  7520,   284,   262,  3999,   329,\n",
       "         517, 10404,    13,   554,   584,  2456,   416,   663,   898,\n",
       "       13938,    11,   262, 10404,   484,  4752,   561,   423,   587,\n",
       "        6699,   284,  1729,    12,    51,   571,   316,   504,    13,\n",
       "         383,  3999,  2391,   550,   284,  2453,   262,  3999,  1570,\n",
       "         326, 18910,   373,   287,   477,  1243,   636,   286,  2807,\n",
       "          13,   843,   523,  2807,  4987,   284,  1309,   262, 18910,\n",
       "         504,  2107,  1231,   262,  3999,  2422,  4931,    13,   198,\n",
       "         198,  3198,   857,   407,   892,   286,   262,   366, 41295,\n",
       "         286,   582,     1,   286,  2807,  1909,   517,   355,   366,\n",
       "       41295,   286,  1181, 12129,  1911,   632,   318,   635,  1593,\n",
       "         329,   262,  3999,   284,  1833,    11,   326,   329,   262,\n",
       "         938,   734, 10675,    11,   355,  2807,   338,   691, 47481,\n",
       "          11,   262, 18910,   504,   423,  1239,   550,   597,  1103,\n",
       "         910,   625,  1181,  1630,    11,   290,   612,   468,   691,\n",
       "         587,   257,  1178, 11557,  3925,    13,   770,   318,  1775,\n",
       "         287,   262,  1109,   326,   867, 18910,   504,  2754,   262,\n",
       "         649,  3999], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [OPTIONAL] Prints the encodings of one of the generated sequences [for reference]\n",
    "generated_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2466026-4f57-4e4a-9e6c-5025ac352046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \tOnce upon a time in China's history the two sides would have had a clear goal of eliminating an independent Tibet, however much China had to concede to the other side. The Chinese would also have had to concede the freedom of Tibet had it been granted to the Chinese for more independence. In other words by its own admission, the independence they claimed would have been denied to non-Tibetans. The Chinese simply had to accept the Chinese view that Tibet was in all things part of China. And so China agreed to let the Tibetans live without the Chinese military presence.\n",
      "\n",
      "One does not think of the \"freedom of man\" of China today more as \"freedom of state capitalism\". It is also important for the Chinese to understand, that for the last two centuries, as China's only superpower, the Tibetans have never had any real say over state control, and there has only been a few isolated individuals. This is seen in the fact that many Tibetans regard the new Chinese\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "2 \tOnce upon a time in China, and again in Europe and South America, the world's largest landlocked landmass was occupied by two continents. (The continent was not part of a separate, separate region. It was both interlocked and not in the same landmass.) And in the course of this land-mining, this huge planet was also inhabited by an interspecies race.\n",
      "\n",
      "It seemed that the same things were happening on a larger scale. But what has happened so far is more than that. It actually started going downhill. During that time, they turned over much of the land on a more frequent scale. They discovered that the land wasn't simply occupied by land masses of many species, for these masses of different populations, that they could use as a resource to generate energy for their entire environment.\n",
      "\n",
      "In many places, land was still a lot less secure than it was even after this land-mining in the 19th century. But now there are things the\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "3 \tOnce upon a time in China it took the United States, Britain and Germany centuries to solve this complicated problem. For an extremely developed and dynamic nation with no obvious foreign policy, China had to be placed where it could live and prosper. On China's doorstep China had to be placed where it was not. It did not have to be part of an island state, but a country at ease with all manner of others. For the United States, or at least those who believed they recognized that, China represented a natural step toward becoming the world's first nation, and for the world it would ultimately prove to be, a source of hope, as it should be. China wasn't the one to build, nor was it the one capable of dealing a blow to others. In fact, it had to be part of the United States rather than with the United Kingdom. Its relationship with the United States was limited to building bridges and improving communication, but never before had so many countries in the globe met as\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "4 \tOnce upon a time in China, the country's economy was so successful that he was forced to flee to Japan. What happened then was that the country's young men finally gained access to a real business world and, as well as their training in the arts, they began to develop real products, even though they lacked an apprenticeship.\n",
      "\n",
      "\n",
      "He became well-known for his creative prowess and for inspiring others to imitate him. During the Cultural Revolution (1950-1980), the main elements of his works were published in magazines like the Evening Standard and the Shanghai Times. For example, he wrote under the title \"The Chinese Revolution: The Last Chance\", the first title he wrote in which he explained his thoughts on history.\n",
      "\n",
      "\n",
      "He did not go into specifics, just that he believed he was the most talented Chinese designer out of the millions of writers and artists in the world. He developed the concept of the \"Great Design Process,\" which was inspired by the Japanese masterwork of the same name\n",
      "-------------------------------------------------------------------------------- \n",
      "\n",
      "5 \tOnce upon a time in China, the United States would have been in a very strong position to try to bring the communist party into control, or else it would have fought it out in a war. It would have been possible, and likely likely succeeded, to intervene, but with a degree of political instability from an authoritarian ruling party. And we have very bad conditions, so it is inevitable that the United States has no way to prevent the worst kind of war.\n",
      "\n",
      "SARTRE: Now, on that last topic, Secretary Clinton has said that we want a comprehensive disarmament of China and they have it now. And we, I think, disagree on whether that means military use of nuclear weapons against our allies or a political solution. Do you agree with her?\n",
      "\n",
      "SECRETARY CLINTON: No. I'm not sure that that would mean a military campaign in which we've said we have to say, \"I don't want a nuclear war, but we've got to\n",
      "-------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decodes all the generated sequences\n",
    "for idx, sequence in enumerate(generated_sequences):\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(idx+1, \"\\t\" + text)\n",
    "    print(\"-\" * 80, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "keras3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
